{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Setting up environment...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.exists('/content/models'):\n",
        "    print(\"Restoring Code...\")\n",
        "    !git clone https://github.com/Gabrysse/MLDL2024_project1.git temp_repo\n",
        "    shutil.copytree('temp_repo/models', '/content/models')\n",
        "    shutil.rmtree('temp_repo')\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/semseg/project_data.zip'\n",
        "\n",
        "if not os.path.exists('/content/dataset/project_data/gta5'):\n",
        "    print(\"Restoring Dataset...\")\n",
        "    if not os.path.exists('/content/dataset'): os.makedirs('/content/dataset')\n",
        "\n",
        "    if os.path.exists(zip_path):\n",
        "        shutil.unpack_archive(zip_path, '/content/dataset')\n",
        "        print(\"Dataset restored.\")\n",
        "    else:\n",
        "        print(f\"Error: Zip file not found at {zip_path}\")\n",
        "else:\n",
        "    print(\"Dataset already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVIh-JRJr39A",
        "outputId": "731ea979-930e-4953-f45a-882feaa47a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up environment...\n",
            "Mounted at /content/drive\n",
            "Restoring Code...\n",
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 481.00 KiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "Restoring Dataset...\n",
            "Dataset restored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image, ImageFilter\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "print(\"üöÄ Starting Step 3b: Training with Augmentations...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure models can be imported\n",
        "if os.path.exists('/content/MLDL2024_project1'):\n",
        "    sys.path.append('/content/MLDL2024_project1')\n",
        "elif not os.path.exists('/content/models'):\n",
        "    # Quick fix if models folder is missing\n",
        "    !git clone https://github.com/Gabrysse/MLDL2024_project1.git temp_repo\n",
        "    import shutil\n",
        "    shutil.copytree('temp_repo/models', '/content/models')\n",
        "\n",
        "# --- 2. AUGMENTED DATASET CLASS ---\n",
        "id_mapping = {\n",
        "    7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "    19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "    26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "}\n",
        "\n",
        "class GTA5AugmentedDataset(Dataset):\n",
        "    def __init__(self, root_dir, augment=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.augment = augment\n",
        "        self.images_dir = os.path.join(root_dir, 'images')\n",
        "        self.masks_dir = os.path.join(root_dir, 'labels')\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "\n",
        "        if os.path.exists(self.images_dir):\n",
        "            for file_name in sorted(os.listdir(self.images_dir)):\n",
        "                self.images.append(os.path.join(self.images_dir, file_name))\n",
        "                self.masks.append(os.path.join(self.masks_dir, file_name))\n",
        "\n",
        "        # Define Photometric Transforms (Image ONLY)\n",
        "        self.color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB')\n",
        "        mask = Image.open(self.masks[idx])\n",
        "\n",
        "        # 1. Base Resize (Project Requirement: 1280x720)\n",
        "        image = image.resize((1280, 720), Image.BILINEAR)\n",
        "        mask = mask.resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        # --- 2. AUGMENTATIONS ---\n",
        "        if self.augment:\n",
        "            # A. Random Horizontal Flip (Geometric - Applied to BOTH)\n",
        "            if random.random() > 0.5:\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            # B. Color Jitter (Photometric - Image ONLY)\n",
        "            if random.random() > 0.5:\n",
        "                image = self.color_jitter(image)\n",
        "\n",
        "            # C. Gaussian Blur (Photometric - Image ONLY)\n",
        "            if random.random() > 0.5:\n",
        "                image = image.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.1, 2.0)))\n",
        "\n",
        "        # 3. Final Formatting\n",
        "        image = self.normalize(image)\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "        target_mask = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in id_mapping.items():\n",
        "            target_mask[mask_np == k] = v\n",
        "\n",
        "        return image, torch.from_numpy(target_mask).long()\n",
        "\n",
        "# --- 3. TRAINING SETUP ---\n",
        "# Important: Create a NEW checkpoint file so we don't overwrite your baseline!\n",
        "checkpoint_path = '/content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth'\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18')\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "\n",
        "if os.path.exists(GTA_PATH):\n",
        "    # Enable augmentations here!\n",
        "    train_dataset = GTA5AugmentedDataset(GTA_PATH, augment=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "\n",
        "    print(f\"üé¨ Starting Training (Augmented)...\")\n",
        "    print(f\"üíæ Saving to: {checkpoint_path}\")\n",
        "\n",
        "    start_epoch = 0\n",
        "    num_epochs = 50 # [cite: 27]\n",
        "\n",
        "    # Resume capability for this new checkpoint\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(\"üîÑ Resuming augmented training...\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            if isinstance(outputs, tuple):\n",
        "                loss = criterion(outputs[0], labels) + 0.1 * criterion(outputs[1], labels) + 0.1 * criterion(outputs[2], labels)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{i}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }, checkpoint_path)\n",
        "        print(f\"‚úÖ Epoch {epoch+1} Saved.\")\n",
        "else:\n",
        "    print(\"‚ùå GTA5 dataset not found. Please unzip project_data.zip again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oF8wUKCpv_-",
        "outputId": "9fc40521-49ec-41bd-fd05-ed4e22de4ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Step 3b: Training with Augmentations...\n",
            "üé¨ Starting Training (Augmented)...\n",
            "üíæ Saving to: /content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth\n",
            "Epoch [1/50] Step [0/313] Loss: 4.3948\n",
            "Epoch [1/50] Step [50/313] Loss: 1.1009\n",
            "Epoch [1/50] Step [100/313] Loss: 0.8189\n",
            "Epoch [1/50] Step [150/313] Loss: 0.6185\n",
            "Epoch [1/50] Step [200/313] Loss: 0.6791\n",
            "Epoch [1/50] Step [250/313] Loss: 0.5196\n",
            "Epoch [1/50] Step [300/313] Loss: 0.5247\n",
            "‚úÖ Epoch 1 Saved.\n",
            "Epoch [2/50] Step [0/313] Loss: 0.5069\n",
            "Epoch [2/50] Step [50/313] Loss: 0.5373\n",
            "Epoch [2/50] Step [100/313] Loss: 0.4711\n",
            "Epoch [2/50] Step [150/313] Loss: 0.4364\n",
            "Epoch [2/50] Step [200/313] Loss: 0.4240\n",
            "Epoch [2/50] Step [250/313] Loss: 0.4166\n",
            "Epoch [2/50] Step [300/313] Loss: 0.4238\n",
            "‚úÖ Epoch 2 Saved.\n",
            "Epoch [3/50] Step [0/313] Loss: 0.4025\n",
            "Epoch [3/50] Step [50/313] Loss: 0.4116\n",
            "Epoch [3/50] Step [100/313] Loss: 0.3659\n",
            "Epoch [3/50] Step [150/313] Loss: 0.3188\n",
            "Epoch [3/50] Step [200/313] Loss: 0.4270\n",
            "Epoch [3/50] Step [250/313] Loss: 0.3799\n",
            "Epoch [3/50] Step [300/313] Loss: 0.3849\n",
            "‚úÖ Epoch 3 Saved.\n",
            "Epoch [4/50] Step [0/313] Loss: 0.4782\n",
            "Epoch [4/50] Step [50/313] Loss: 0.3675\n",
            "Epoch [4/50] Step [100/313] Loss: 0.3761\n",
            "Epoch [4/50] Step [150/313] Loss: 0.3585\n",
            "Epoch [4/50] Step [200/313] Loss: 0.3758\n",
            "Epoch [4/50] Step [250/313] Loss: 0.3932\n",
            "Epoch [4/50] Step [300/313] Loss: 0.3587\n",
            "‚úÖ Epoch 4 Saved.\n",
            "Epoch [5/50] Step [0/313] Loss: 0.3789\n",
            "Epoch [5/50] Step [50/313] Loss: 0.3016\n",
            "Epoch [5/50] Step [100/313] Loss: 0.3831\n",
            "Epoch [5/50] Step [150/313] Loss: 0.3254\n",
            "Epoch [5/50] Step [200/313] Loss: 0.3300\n",
            "Epoch [5/50] Step [250/313] Loss: 0.3124\n",
            "Epoch [5/50] Step [300/313] Loss: 0.3237\n",
            "‚úÖ Epoch 5 Saved.\n",
            "Epoch [6/50] Step [0/313] Loss: 0.2960\n",
            "Epoch [6/50] Step [50/313] Loss: 0.3535\n",
            "Epoch [6/50] Step [100/313] Loss: 0.2920\n",
            "Epoch [6/50] Step [150/313] Loss: 0.3120\n",
            "Epoch [6/50] Step [200/313] Loss: 0.2843\n",
            "Epoch [6/50] Step [250/313] Loss: 0.3105\n",
            "Epoch [6/50] Step [300/313] Loss: 0.3345\n",
            "‚úÖ Epoch 6 Saved.\n",
            "Epoch [7/50] Step [0/313] Loss: 0.3020\n",
            "Epoch [7/50] Step [50/313] Loss: 0.3183\n",
            "Epoch [7/50] Step [100/313] Loss: 0.2973\n",
            "Epoch [7/50] Step [150/313] Loss: 0.2497\n",
            "Epoch [7/50] Step [200/313] Loss: 0.3192\n",
            "Epoch [7/50] Step [250/313] Loss: 0.2876\n",
            "Epoch [7/50] Step [300/313] Loss: 0.3068\n",
            "‚úÖ Epoch 7 Saved.\n",
            "Epoch [8/50] Step [0/313] Loss: 0.2808\n",
            "Epoch [8/50] Step [50/313] Loss: 0.2711\n",
            "Epoch [8/50] Step [100/313] Loss: 0.2367\n",
            "Epoch [8/50] Step [150/313] Loss: 0.3086\n",
            "Epoch [8/50] Step [200/313] Loss: 0.2903\n",
            "Epoch [8/50] Step [250/313] Loss: 0.2679\n",
            "Epoch [8/50] Step [300/313] Loss: 0.3272\n",
            "‚úÖ Epoch 8 Saved.\n",
            "Epoch [9/50] Step [0/313] Loss: 0.3427\n",
            "Epoch [9/50] Step [50/313] Loss: 0.3132\n",
            "Epoch [9/50] Step [100/313] Loss: 0.2359\n",
            "Epoch [9/50] Step [150/313] Loss: 0.2638\n",
            "Epoch [9/50] Step [200/313] Loss: 0.2392\n",
            "Epoch [9/50] Step [250/313] Loss: 0.2844\n",
            "Epoch [9/50] Step [300/313] Loss: 0.3577\n",
            "‚úÖ Epoch 9 Saved.\n",
            "Epoch [10/50] Step [0/313] Loss: 0.2765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image, ImageFilter\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MOUNT DRIVE & RESTORE ENVIRONMENT ---\n",
        "print(\"üöÄ Initiating Auto-Resume Sequence...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# A. Restore Code\n",
        "if not os.path.exists('/content/models'):\n",
        "    print(\"‚ö†Ô∏è Code folder missing. Restoring...\")\n",
        "    !git clone https://github.com/Gabrysse/MLDL2024_project1.git temp_repo\n",
        "    shutil.copytree('temp_repo/models', '/content/models')\n",
        "    shutil.rmtree('temp_repo')\n",
        "    print(\"‚úÖ Code restored to /content/models\")\n",
        "\n",
        "# B. Restore Dataset\n",
        "if not os.path.exists('/content/dataset/project_data/gta5'):\n",
        "    print(\"‚ö†Ô∏è Dataset missing. Unzipping again... (This takes ~2 mins)\")\n",
        "    if not os.path.exists('/content/dataset'): os.makedirs('/content/dataset')\n",
        "\n",
        "    # Path to your zip file (Adjust if you moved it)\n",
        "    zip_path = '/content/drive/MyDrive/semseg/project_data.zip'\n",
        "\n",
        "    if os.path.exists(zip_path):\n",
        "        shutil.unpack_archive(zip_path, '/content/dataset')\n",
        "        print(\"‚úÖ Dataset restored!\")\n",
        "    else:\n",
        "        print(f\"‚ùå CRITICAL: Could not find zip at {zip_path}\")\n",
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# --- 2. DEFINE AUGMENTED DATASET ---\n",
        "# We must redefine this class so the script can run standalone\n",
        "id_mapping = {\n",
        "    7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "    19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "    26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "}\n",
        "\n",
        "class GTA5AugmentedDataset(Dataset):\n",
        "    def __init__(self, root_dir, augment=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.augment = augment\n",
        "        self.images_dir = os.path.join(root_dir, 'images')\n",
        "        self.masks_dir = os.path.join(root_dir, 'labels')\n",
        "        self.images = sorted(os.listdir(self.images_dir))\n",
        "        self.color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "        mask = Image.open(mask_path).resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        if self.augment:\n",
        "            # Aug 1: Flip (Geometric)\n",
        "            if random.random() > 0.5:\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            # Aug 2: Color/Blur (Photometric)\n",
        "            if random.random() > 0.5:\n",
        "                image = self.color_jitter(image)\n",
        "            if random.random() > 0.5:\n",
        "                image = image.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.1, 2.0)))\n",
        "\n",
        "        image = self.normalize(image)\n",
        "        mask_np = np.array(mask)\n",
        "        target_mask = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in id_mapping.items(): target_mask[mask_np == k] = v\n",
        "        return image, torch.from_numpy(target_mask).long()\n",
        "\n",
        "# --- 3. RESUME TRAINING ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚öôÔ∏è Device: {device}\")\n",
        "\n",
        "# THIS IS THE CHECKPOINT WE WANT TO RESUME\n",
        "checkpoint_path = '/content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth'\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18')\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "start_epoch = 0\n",
        "num_epochs = 50\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"üîÑ Found Checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"‚è© Resuming from Epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"üÜï No checkpoint found. Starting from scratch (Epoch 0)!\")\n",
        "\n",
        "# Setup Loader\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "if os.path.exists(GTA_PATH):\n",
        "    train_dataset = GTA5AugmentedDataset(GTA_PATH, augment=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "\n",
        "    print(\"üé¨ Action: Training Resumed!\")\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            if isinstance(outputs, tuple):\n",
        "                loss = criterion(outputs[0], labels) + 0.1 * criterion(outputs[1], labels) + 0.1 * criterion(outputs[2], labels)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{i}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Save every epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }, checkpoint_path)\n",
        "        print(f\"‚úÖ Epoch {epoch+1} Saved.\")\n",
        "else:\n",
        "    print(\"‚ùå Dataset path not valid. Unzip failed?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgXfVEctsuBB",
        "outputId": "c836e0a1-f147-40fd-a5ec-56132c0b80eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initiating Auto-Resume Sequence...\n",
            "Mounted at /content/drive\n",
            "‚ö†Ô∏è Code folder missing. Restoring...\n",
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "‚úÖ Code restored to /content/models\n",
            "‚ö†Ô∏è Dataset missing. Unzipping again... (This takes ~2 mins)\n",
            "‚úÖ Dataset restored!\n",
            "‚öôÔ∏è Device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 150MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171M/171M [00:01<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Found Checkpoint: /content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth\n",
            "‚è© Resuming from Epoch 9\n",
            "üé¨ Action: Training Resumed!\n",
            "Epoch [10/50] Step [0/313] Loss: 0.2370\n",
            "Epoch [10/50] Step [50/313] Loss: 0.2997\n",
            "Epoch [10/50] Step [100/313] Loss: 0.2620\n",
            "Epoch [10/50] Step [150/313] Loss: 0.3085\n",
            "Epoch [10/50] Step [200/313] Loss: 0.2116\n",
            "Epoch [10/50] Step [250/313] Loss: 0.2866\n",
            "Epoch [10/50] Step [300/313] Loss: 0.2285\n",
            "‚úÖ Epoch 10 Saved.\n",
            "Epoch [11/50] Step [0/313] Loss: 0.2464\n",
            "Epoch [11/50] Step [50/313] Loss: 0.2484\n",
            "Epoch [11/50] Step [100/313] Loss: 0.2632\n",
            "Epoch [11/50] Step [150/313] Loss: 0.2352\n",
            "Epoch [11/50] Step [200/313] Loss: 0.2257\n",
            "Epoch [11/50] Step [250/313] Loss: 0.2542\n",
            "Epoch [11/50] Step [300/313] Loss: 0.2487\n",
            "‚úÖ Epoch 11 Saved.\n",
            "Epoch [12/50] Step [0/313] Loss: 0.2879\n",
            "Epoch [12/50] Step [50/313] Loss: 0.2414\n",
            "Epoch [12/50] Step [100/313] Loss: 0.2211\n",
            "Epoch [12/50] Step [150/313] Loss: 0.2776\n",
            "Epoch [12/50] Step [200/313] Loss: 0.2714\n",
            "Epoch [12/50] Step [250/313] Loss: 0.2300\n",
            "Epoch [12/50] Step [300/313] Loss: 0.2346\n",
            "‚úÖ Epoch 12 Saved.\n",
            "Epoch [13/50] Step [0/313] Loss: 0.2226\n",
            "Epoch [13/50] Step [50/313] Loss: 0.1972\n",
            "Epoch [13/50] Step [100/313] Loss: 0.1907\n",
            "Epoch [13/50] Step [150/313] Loss: 0.2252\n",
            "Epoch [13/50] Step [200/313] Loss: 0.2801\n",
            "Epoch [13/50] Step [250/313] Loss: 0.3035\n",
            "Epoch [13/50] Step [300/313] Loss: 0.2013\n",
            "‚úÖ Epoch 13 Saved.\n",
            "Epoch [14/50] Step [0/313] Loss: 0.2248\n",
            "Epoch [14/50] Step [50/313] Loss: 0.2457\n",
            "Epoch [14/50] Step [100/313] Loss: 0.2458\n",
            "Epoch [14/50] Step [150/313] Loss: 0.2221\n",
            "Epoch [14/50] Step [200/313] Loss: 0.2616\n",
            "Epoch [14/50] Step [250/313] Loss: 0.2687\n",
            "Epoch [14/50] Step [300/313] Loss: 0.2410\n",
            "‚úÖ Epoch 14 Saved.\n",
            "Epoch [15/50] Step [0/313] Loss: 0.2702\n",
            "Epoch [15/50] Step [50/313] Loss: 0.2236\n",
            "Epoch [15/50] Step [100/313] Loss: 0.2121\n",
            "Epoch [15/50] Step [150/313] Loss: 0.2469\n",
            "Epoch [15/50] Step [200/313] Loss: 0.2485\n",
            "Epoch [15/50] Step [250/313] Loss: 0.2497\n",
            "Epoch [15/50] Step [300/313] Loss: 0.2509\n",
            "‚úÖ Epoch 15 Saved.\n",
            "Epoch [16/50] Step [0/313] Loss: 0.2364\n",
            "Epoch [16/50] Step [50/313] Loss: 0.2092\n",
            "Epoch [16/50] Step [100/313] Loss: 0.2624\n",
            "Epoch [16/50] Step [150/313] Loss: 0.2020\n",
            "Epoch [16/50] Step [200/313] Loss: 0.2305\n",
            "Epoch [16/50] Step [250/313] Loss: 0.2209\n",
            "Epoch [16/50] Step [300/313] Loss: 0.2239\n",
            "‚úÖ Epoch 16 Saved.\n",
            "Epoch [17/50] Step [0/313] Loss: 0.2541\n",
            "Epoch [17/50] Step [50/313] Loss: 0.2149\n",
            "Epoch [17/50] Step [100/313] Loss: 0.2091\n",
            "Epoch [17/50] Step [150/313] Loss: 0.2001\n",
            "Epoch [17/50] Step [200/313] Loss: 0.2263\n",
            "Epoch [17/50] Step [250/313] Loss: 0.1899\n",
            "Epoch [17/50] Step [300/313] Loss: 0.2144\n",
            "‚úÖ Epoch 17 Saved.\n",
            "Epoch [18/50] Step [0/313] Loss: 0.2200\n",
            "Epoch [18/50] Step [50/313] Loss: 0.1935\n",
            "Epoch [18/50] Step [100/313] Loss: 0.2422\n",
            "Epoch [18/50] Step [150/313] Loss: 0.2190\n",
            "Epoch [18/50] Step [200/313] Loss: 0.2133\n",
            "Epoch [18/50] Step [250/313] Loss: 0.2345\n",
            "Epoch [18/50] Step [300/313] Loss: 0.2489\n",
            "‚úÖ Epoch 18 Saved.\n",
            "Epoch [19/50] Step [0/313] Loss: 0.2295\n",
            "Epoch [19/50] Step [50/313] Loss: 0.1985\n",
            "Epoch [19/50] Step [100/313] Loss: 0.2267\n",
            "Epoch [19/50] Step [150/313] Loss: 0.2081\n",
            "Epoch [19/50] Step [200/313] Loss: 0.2456\n",
            "Epoch [19/50] Step [250/313] Loss: 0.2266\n",
            "Epoch [19/50] Step [300/313] Loss: 0.2537\n",
            "‚úÖ Epoch 19 Saved.\n",
            "Epoch [20/50] Step [0/313] Loss: 0.2639\n",
            "Epoch [20/50] Step [50/313] Loss: 0.1893\n",
            "Epoch [20/50] Step [100/313] Loss: 0.2251\n",
            "Epoch [20/50] Step [150/313] Loss: 0.2170\n",
            "Epoch [20/50] Step [200/313] Loss: 0.2094\n",
            "Epoch [20/50] Step [250/313] Loss: 0.2475\n",
            "Epoch [20/50] Step [300/313] Loss: 0.2413\n",
            "‚úÖ Epoch 20 Saved.\n",
            "Epoch [21/50] Step [0/313] Loss: 0.2521\n",
            "Epoch [21/50] Step [50/313] Loss: 0.2339\n",
            "Epoch [21/50] Step [100/313] Loss: 0.2379\n",
            "Epoch [21/50] Step [150/313] Loss: 0.2249\n",
            "Epoch [21/50] Step [200/313] Loss: 0.2091\n",
            "Epoch [21/50] Step [250/313] Loss: 0.2640\n",
            "Epoch [21/50] Step [300/313] Loss: 0.4680\n",
            "‚úÖ Epoch 21 Saved.\n",
            "Epoch [22/50] Step [0/313] Loss: 0.3599\n",
            "Epoch [22/50] Step [50/313] Loss: 0.2265\n",
            "Epoch [22/50] Step [100/313] Loss: 0.2136\n",
            "Epoch [22/50] Step [150/313] Loss: 0.2072\n",
            "Epoch [22/50] Step [200/313] Loss: 0.2535\n",
            "Epoch [22/50] Step [250/313] Loss: 0.2672\n",
            "Epoch [22/50] Step [300/313] Loss: 0.2008\n",
            "‚úÖ Epoch 22 Saved.\n",
            "Epoch [23/50] Step [0/313] Loss: 0.2266\n",
            "Epoch [23/50] Step [50/313] Loss: 0.2252\n",
            "Epoch [23/50] Step [100/313] Loss: 0.1869\n",
            "Epoch [23/50] Step [150/313] Loss: 0.2061\n",
            "Epoch [23/50] Step [200/313] Loss: 0.2331\n",
            "Epoch [23/50] Step [250/313] Loss: 0.2545\n",
            "Epoch [23/50] Step [300/313] Loss: 0.2438\n",
            "‚úÖ Epoch 23 Saved.\n",
            "Epoch [24/50] Step [0/313] Loss: 0.2513\n",
            "Epoch [24/50] Step [50/313] Loss: 0.2477\n",
            "Epoch [24/50] Step [100/313] Loss: 0.2377\n",
            "Epoch [24/50] Step [150/313] Loss: 0.1970\n",
            "Epoch [24/50] Step [200/313] Loss: 0.2044\n",
            "Epoch [24/50] Step [250/313] Loss: 0.2088\n",
            "Epoch [24/50] Step [300/313] Loss: 0.2059\n",
            "‚úÖ Epoch 24 Saved.\n",
            "Epoch [25/50] Step [0/313] Loss: 0.2079\n",
            "Epoch [25/50] Step [50/313] Loss: 0.1996\n",
            "Epoch [25/50] Step [100/313] Loss: 0.2137\n",
            "Epoch [25/50] Step [150/313] Loss: 0.1816\n",
            "Epoch [25/50] Step [200/313] Loss: 0.1937\n",
            "Epoch [25/50] Step [250/313] Loss: 0.1992\n",
            "Epoch [25/50] Step [300/313] Loss: 0.2057\n",
            "‚úÖ Epoch 25 Saved.\n",
            "Epoch [26/50] Step [0/313] Loss: 0.1884\n",
            "Epoch [26/50] Step [50/313] Loss: 0.2068\n",
            "Epoch [26/50] Step [100/313] Loss: 0.1895\n",
            "Epoch [26/50] Step [150/313] Loss: 0.2262\n",
            "Epoch [26/50] Step [200/313] Loss: 0.1594\n",
            "Epoch [26/50] Step [250/313] Loss: 0.1885\n",
            "Epoch [26/50] Step [300/313] Loss: 0.1965\n",
            "‚úÖ Epoch 26 Saved.\n",
            "Epoch [27/50] Step [0/313] Loss: 0.1481\n",
            "Epoch [27/50] Step [50/313] Loss: 0.1918\n",
            "Epoch [27/50] Step [100/313] Loss: 0.1973\n",
            "Epoch [27/50] Step [150/313] Loss: 0.2061\n",
            "Epoch [27/50] Step [200/313] Loss: 0.1702\n",
            "Epoch [27/50] Step [250/313] Loss: 0.2018\n",
            "Epoch [27/50] Step [300/313] Loss: 0.1911\n",
            "‚úÖ Epoch 27 Saved.\n",
            "Epoch [28/50] Step [0/313] Loss: 0.1885\n",
            "Epoch [28/50] Step [50/313] Loss: 0.1894\n",
            "Epoch [28/50] Step [100/313] Loss: 0.1743\n",
            "Epoch [28/50] Step [150/313] Loss: 0.1935\n",
            "Epoch [28/50] Step [200/313] Loss: 0.2020\n",
            "Epoch [28/50] Step [250/313] Loss: 0.2002\n",
            "Epoch [28/50] Step [300/313] Loss: 0.1933\n",
            "‚úÖ Epoch 28 Saved.\n",
            "Epoch [29/50] Step [0/313] Loss: 0.2333\n",
            "Epoch [29/50] Step [50/313] Loss: 0.1652\n",
            "Epoch [29/50] Step [100/313] Loss: 0.2084\n",
            "Epoch [29/50] Step [150/313] Loss: 0.1958\n",
            "Epoch [29/50] Step [200/313] Loss: 0.1833\n",
            "Epoch [29/50] Step [250/313] Loss: 0.2128\n",
            "Epoch [29/50] Step [300/313] Loss: 0.2057\n",
            "‚úÖ Epoch 29 Saved.\n",
            "Epoch [30/50] Step [0/313] Loss: 0.1797\n",
            "Epoch [30/50] Step [50/313] Loss: 0.1674\n",
            "Epoch [30/50] Step [100/313] Loss: 0.1898\n",
            "Epoch [30/50] Step [150/313] Loss: 0.1756\n",
            "Epoch [30/50] Step [200/313] Loss: 0.1737\n",
            "Epoch [30/50] Step [250/313] Loss: 0.1946\n",
            "Epoch [30/50] Step [300/313] Loss: 0.1778\n",
            "‚úÖ Epoch 30 Saved.\n",
            "Epoch [31/50] Step [0/313] Loss: 0.1959\n",
            "Epoch [31/50] Step [50/313] Loss: 0.3388\n",
            "Epoch [31/50] Step [100/313] Loss: 0.2711\n",
            "Epoch [31/50] Step [150/313] Loss: 0.1767\n",
            "Epoch [31/50] Step [200/313] Loss: 0.1889\n",
            "Epoch [31/50] Step [250/313] Loss: 0.2266\n",
            "Epoch [31/50] Step [300/313] Loss: 0.3017\n",
            "‚úÖ Epoch 31 Saved.\n",
            "Epoch [32/50] Step [0/313] Loss: 0.2091\n",
            "Epoch [32/50] Step [50/313] Loss: 0.1979\n",
            "Epoch [32/50] Step [100/313] Loss: 0.1894\n",
            "Epoch [32/50] Step [150/313] Loss: 0.1862\n",
            "Epoch [32/50] Step [200/313] Loss: 0.2111\n",
            "Epoch [32/50] Step [250/313] Loss: 0.1612\n",
            "Epoch [32/50] Step [300/313] Loss: 0.1590\n",
            "‚úÖ Epoch 32 Saved.\n",
            "Epoch [33/50] Step [0/313] Loss: 0.1928\n",
            "Epoch [33/50] Step [50/313] Loss: 0.1822\n",
            "Epoch [33/50] Step [100/313] Loss: 0.2028\n",
            "Epoch [33/50] Step [150/313] Loss: 0.1857\n",
            "Epoch [33/50] Step [200/313] Loss: 0.2155\n",
            "Epoch [33/50] Step [250/313] Loss: 0.1856\n",
            "Epoch [33/50] Step [300/313] Loss: 0.1872\n",
            "‚úÖ Epoch 33 Saved.\n",
            "Epoch [34/50] Step [0/313] Loss: 0.1780\n",
            "Epoch [34/50] Step [50/313] Loss: 0.1616\n",
            "Epoch [34/50] Step [100/313] Loss: 0.1652\n",
            "Epoch [34/50] Step [150/313] Loss: 0.1899\n",
            "Epoch [34/50] Step [200/313] Loss: 0.2147\n",
            "Epoch [34/50] Step [250/313] Loss: 0.1896\n",
            "Epoch [34/50] Step [300/313] Loss: 0.1859\n",
            "‚úÖ Epoch 34 Saved.\n",
            "Epoch [35/50] Step [0/313] Loss: 0.2256\n",
            "Epoch [35/50] Step [50/313] Loss: 0.1583\n",
            "Epoch [35/50] Step [100/313] Loss: 0.2000\n",
            "Epoch [35/50] Step [150/313] Loss: 0.1695\n",
            "Epoch [35/50] Step [200/313] Loss: 0.1519\n",
            "Epoch [35/50] Step [250/313] Loss: 0.1971\n",
            "Epoch [35/50] Step [300/313] Loss: 0.1859\n",
            "‚úÖ Epoch 35 Saved.\n",
            "Epoch [36/50] Step [0/313] Loss: 0.1827\n",
            "Epoch [36/50] Step [50/313] Loss: 0.1793\n",
            "Epoch [36/50] Step [100/313] Loss: 0.1762\n",
            "Epoch [36/50] Step [150/313] Loss: 0.2143\n",
            "Epoch [36/50] Step [200/313] Loss: 0.1937\n",
            "Epoch [36/50] Step [250/313] Loss: 0.1895\n",
            "Epoch [36/50] Step [300/313] Loss: 0.2195\n",
            "‚úÖ Epoch 36 Saved.\n",
            "Epoch [37/50] Step [0/313] Loss: 0.1882\n",
            "Epoch [37/50] Step [50/313] Loss: 0.1975\n",
            "Epoch [37/50] Step [100/313] Loss: 0.1653\n",
            "Epoch [37/50] Step [150/313] Loss: 0.1590\n",
            "Epoch [37/50] Step [200/313] Loss: 0.1768\n",
            "Epoch [37/50] Step [250/313] Loss: 0.1685\n",
            "Epoch [37/50] Step [300/313] Loss: 0.1675\n",
            "‚úÖ Epoch 37 Saved.\n",
            "Epoch [38/50] Step [0/313] Loss: 0.1675\n",
            "Epoch [38/50] Step [50/313] Loss: 0.2103\n",
            "Epoch [38/50] Step [100/313] Loss: 0.1554\n",
            "Epoch [38/50] Step [150/313] Loss: 0.1771\n",
            "Epoch [38/50] Step [200/313] Loss: 0.2040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image, ImageFilter\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MOUNT DRIVE & RESTORE ENVIRONMENT ---\n",
        "print(\"üöÄ Initiating Auto-Resume Sequence...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# A. Restore Code\n",
        "if not os.path.exists('/content/models'):\n",
        "    print(\"‚ö†Ô∏è Code folder missing. Restoring...\")\n",
        "    !git clone https://github.com/Gabrysse/MLDL2024_project1.git temp_repo\n",
        "    shutil.copytree('temp_repo/models', '/content/models')\n",
        "    shutil.rmtree('temp_repo')\n",
        "    print(\"‚úÖ Code restored to /content/models\")\n",
        "\n",
        "# B. Restore Dataset\n",
        "if not os.path.exists('/content/dataset/project_data/gta5'):\n",
        "    print(\"‚ö†Ô∏è Dataset missing. Unzipping again... (This takes ~2 mins)\")\n",
        "    if not os.path.exists('/content/dataset'): os.makedirs('/content/dataset')\n",
        "\n",
        "    # Path to your zip file (Adjust if you moved it)\n",
        "    zip_path = '/content/drive/MyDrive/semseg/project_data.zip'\n",
        "\n",
        "    if os.path.exists(zip_path):\n",
        "        shutil.unpack_archive(zip_path, '/content/dataset')\n",
        "        print(\"‚úÖ Dataset restored!\")\n",
        "    else:\n",
        "        print(f\"‚ùå CRITICAL: Could not find zip at {zip_path}\")\n",
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# --- 2. DEFINE AUGMENTED DATASET ---\n",
        "# We must redefine this class so the script can run standalone\n",
        "id_mapping = {\n",
        "    7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "    19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "    26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "}\n",
        "\n",
        "class GTA5AugmentedDataset(Dataset):\n",
        "    def __init__(self, root_dir, augment=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.augment = augment\n",
        "        self.images_dir = os.path.join(root_dir, 'images')\n",
        "        self.masks_dir = os.path.join(root_dir, 'labels')\n",
        "        self.images = sorted(os.listdir(self.images_dir))\n",
        "        self.color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "        mask = Image.open(mask_path).resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        if self.augment:\n",
        "            # Aug 1: Flip (Geometric)\n",
        "            if random.random() > 0.5:\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            # Aug 2: Color/Blur (Photometric)\n",
        "            if random.random() > 0.5:\n",
        "                image = self.color_jitter(image)\n",
        "            if random.random() > 0.5:\n",
        "                image = image.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.1, 2.0)))\n",
        "\n",
        "        image = self.normalize(image)\n",
        "        mask_np = np.array(mask)\n",
        "        target_mask = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in id_mapping.items(): target_mask[mask_np == k] = v\n",
        "        return image, torch.from_numpy(target_mask).long()\n",
        "\n",
        "# --- 3. RESUME TRAINING ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚öôÔ∏è Device: {device}\")\n",
        "\n",
        "# THIS IS THE CHECKPOINT WE WANT TO RESUME\n",
        "checkpoint_path = '/content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth'\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18')\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "start_epoch = 0\n",
        "num_epochs = 50\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"üîÑ Found Checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"‚è© Resuming from Epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"üÜï No checkpoint found. Starting from scratch (Epoch 0)!\")\n",
        "\n",
        "# Setup Loader\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "if os.path.exists(GTA_PATH):\n",
        "    train_dataset = GTA5AugmentedDataset(GTA_PATH, augment=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "\n",
        "    print(\"üé¨ Action: Training Resumed!\")\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            if isinstance(outputs, tuple):\n",
        "                loss = criterion(outputs[0], labels) + 0.1 * criterion(outputs[1], labels) + 0.1 * criterion(outputs[2], labels)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{i}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Save every epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }, checkpoint_path)\n",
        "        print(f\"‚úÖ Epoch {epoch+1} Saved.\")\n",
        "else:\n",
        "    print(\"‚ùå Dataset path not valid. Unzip failed?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff87e595-a035-4a7f-dfbe-0d98e9d3203b",
        "id": "rl8i5HZsLzeG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initiating Auto-Resume Sequence...\n",
            "Mounted at /content/drive\n",
            "‚ö†Ô∏è Code folder missing. Restoring...\n",
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "‚úÖ Code restored to /content/models\n",
            "‚ö†Ô∏è Dataset missing. Unzipping again... (This takes ~2 mins)\n",
            "‚úÖ Dataset restored!\n",
            "‚öôÔ∏è Device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 56.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171M/171M [00:01<00:00, 129MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Found Checkpoint: /content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth\n",
            "‚è© Resuming from Epoch 37\n",
            "üé¨ Action: Training Resumed!\n",
            "Epoch [38/50] Step [0/313] Loss: 0.1682\n",
            "Epoch [38/50] Step [50/313] Loss: 0.1686\n",
            "Epoch [38/50] Step [100/313] Loss: 0.2043\n",
            "Epoch [38/50] Step [150/313] Loss: 0.1913\n",
            "Epoch [38/50] Step [200/313] Loss: 0.1645\n",
            "Epoch [38/50] Step [250/313] Loss: 0.1737\n",
            "Epoch [38/50] Step [300/313] Loss: 0.1880\n",
            "‚úÖ Epoch 38 Saved.\n",
            "Epoch [39/50] Step [0/313] Loss: 0.1575\n",
            "Epoch [39/50] Step [50/313] Loss: 0.2051\n",
            "Epoch [39/50] Step [100/313] Loss: 0.2305\n",
            "Epoch [39/50] Step [150/313] Loss: 0.2143\n",
            "Epoch [39/50] Step [200/313] Loss: 0.1748\n",
            "Epoch [39/50] Step [250/313] Loss: 0.1766\n",
            "Epoch [39/50] Step [300/313] Loss: 0.1600\n",
            "‚úÖ Epoch 39 Saved.\n",
            "Epoch [40/50] Step [0/313] Loss: 0.1773\n",
            "Epoch [40/50] Step [50/313] Loss: 0.1778\n",
            "Epoch [40/50] Step [100/313] Loss: 0.2187\n",
            "Epoch [40/50] Step [150/313] Loss: 0.1967\n",
            "Epoch [40/50] Step [200/313] Loss: 0.1832\n",
            "Epoch [40/50] Step [250/313] Loss: 0.2076\n",
            "Epoch [40/50] Step [300/313] Loss: 0.1953\n",
            "‚úÖ Epoch 40 Saved.\n",
            "Epoch [41/50] Step [0/313] Loss: 0.1850\n",
            "Epoch [41/50] Step [50/313] Loss: 0.2084\n",
            "Epoch [41/50] Step [100/313] Loss: 0.1671\n",
            "Epoch [41/50] Step [150/313] Loss: 0.1856\n",
            "Epoch [41/50] Step [200/313] Loss: 0.1651\n",
            "Epoch [41/50] Step [250/313] Loss: 0.1573\n",
            "Epoch [41/50] Step [300/313] Loss: 0.1830\n",
            "‚úÖ Epoch 41 Saved.\n",
            "Epoch [42/50] Step [0/313] Loss: 0.1400\n",
            "Epoch [42/50] Step [50/313] Loss: 0.1829\n",
            "Epoch [42/50] Step [100/313] Loss: 0.1618\n",
            "Epoch [42/50] Step [150/313] Loss: 0.1792\n",
            "Epoch [42/50] Step [200/313] Loss: 0.1970\n",
            "Epoch [42/50] Step [250/313] Loss: 0.1756\n",
            "Epoch [42/50] Step [300/313] Loss: 0.2013\n",
            "‚úÖ Epoch 42 Saved.\n",
            "Epoch [43/50] Step [0/313] Loss: 0.1368\n",
            "Epoch [43/50] Step [50/313] Loss: 0.1657\n",
            "Epoch [43/50] Step [100/313] Loss: 0.1620\n",
            "Epoch [43/50] Step [150/313] Loss: 0.1477\n",
            "Epoch [43/50] Step [200/313] Loss: 0.1584\n",
            "Epoch [43/50] Step [250/313] Loss: 0.1601\n",
            "Epoch [43/50] Step [300/313] Loss: 0.1699\n",
            "‚úÖ Epoch 43 Saved.\n",
            "Epoch [44/50] Step [0/313] Loss: 0.1471\n",
            "Epoch [44/50] Step [50/313] Loss: 0.2103\n",
            "Epoch [44/50] Step [100/313] Loss: 0.1948\n",
            "Epoch [44/50] Step [150/313] Loss: 0.1472\n",
            "Epoch [44/50] Step [200/313] Loss: 0.1873\n",
            "Epoch [44/50] Step [250/313] Loss: 0.1737\n",
            "Epoch [44/50] Step [300/313] Loss: 0.1820\n",
            "‚úÖ Epoch 44 Saved.\n",
            "Epoch [45/50] Step [0/313] Loss: 0.1479\n",
            "Epoch [45/50] Step [50/313] Loss: 0.1530\n",
            "Epoch [45/50] Step [100/313] Loss: 0.1831\n",
            "Epoch [45/50] Step [150/313] Loss: 0.1707\n",
            "Epoch [45/50] Step [200/313] Loss: 0.1580\n",
            "Epoch [45/50] Step [250/313] Loss: 0.1614\n",
            "Epoch [45/50] Step [300/313] Loss: 0.1865\n",
            "‚úÖ Epoch 45 Saved.\n",
            "Epoch [46/50] Step [0/313] Loss: 0.1965\n",
            "Epoch [46/50] Step [50/313] Loss: 0.1661\n",
            "Epoch [46/50] Step [100/313] Loss: 0.1679\n",
            "Epoch [46/50] Step [150/313] Loss: 0.2127\n",
            "Epoch [46/50] Step [200/313] Loss: 0.2036\n",
            "Epoch [46/50] Step [250/313] Loss: 0.1788\n",
            "Epoch [46/50] Step [300/313] Loss: 0.1664\n",
            "‚úÖ Epoch 46 Saved.\n",
            "Epoch [47/50] Step [0/313] Loss: 0.1623\n",
            "Epoch [47/50] Step [50/313] Loss: 0.2233\n",
            "Epoch [47/50] Step [100/313] Loss: 0.1573\n",
            "Epoch [47/50] Step [150/313] Loss: 0.1862\n",
            "Epoch [47/50] Step [200/313] Loss: 0.1790\n",
            "Epoch [47/50] Step [250/313] Loss: 0.1801\n",
            "Epoch [47/50] Step [300/313] Loss: 0.1888\n",
            "‚úÖ Epoch 47 Saved.\n",
            "Epoch [48/50] Step [0/313] Loss: 0.1763\n",
            "Epoch [48/50] Step [50/313] Loss: 0.2257\n",
            "Epoch [48/50] Step [100/313] Loss: 0.1899\n",
            "Epoch [48/50] Step [150/313] Loss: 0.1729\n",
            "Epoch [48/50] Step [200/313] Loss: 0.2149\n",
            "Epoch [48/50] Step [250/313] Loss: 0.2051\n",
            "Epoch [48/50] Step [300/313] Loss: 0.1911\n",
            "‚úÖ Epoch 48 Saved.\n",
            "Epoch [49/50] Step [0/313] Loss: 0.1591\n",
            "Epoch [49/50] Step [50/313] Loss: 0.1755\n",
            "Epoch [49/50] Step [100/313] Loss: 0.2012\n",
            "Epoch [49/50] Step [150/313] Loss: 0.1906\n",
            "Epoch [49/50] Step [200/313] Loss: 0.1869\n",
            "Epoch [49/50] Step [250/313] Loss: 0.1997\n",
            "Epoch [49/50] Step [300/313] Loss: 0.1754\n",
            "‚úÖ Epoch 49 Saved.\n",
            "Epoch [50/50] Step [0/313] Loss: 0.1659\n",
            "Epoch [50/50] Step [50/313] Loss: 0.1808\n",
            "Epoch [50/50] Step [100/313] Loss: 0.1476\n",
            "Epoch [50/50] Step [150/313] Loss: 0.1516\n",
            "Epoch [50/50] Step [200/313] Loss: 0.1584\n",
            "Epoch [50/50] Step [250/313] Loss: 0.1583\n",
            "Epoch [50/50] Step [300/313] Loss: 0.1812\n",
            "‚úÖ Epoch 50 Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "# ‚úÖ TESTING: Combined Augmentation Model\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth'\n",
        "NUM_CLASSES = 19\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Class Names in correct order\n",
        "CLASSES = [\n",
        "    \"Road\", \"Sidewalk\", \"Building\", \"Wall\", \"Fence\", \"Pole\",\n",
        "    \"Traffic Light\", \"Traffic Sign\", \"Vegetation\", \"Terrain\", \"Sky\",\n",
        "    \"Person\", \"Rider\", \"Car\", \"Truck\", \"Bus\", \"Train\", \"Motorcycle\", \"Bicycle\"\n",
        "]\n",
        "\n",
        "if os.path.exists('/content/MLDL2024_project1'): sys.path.append('/content/MLDL2024_project1')\n",
        "\n",
        "# --- DATASET ---\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root, split='val', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.images_dir = os.path.join(root, 'leftImg8bit', split)\n",
        "        self.masks_dir = os.path.join(root, 'gtFine', split)\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "\n",
        "        if os.path.exists(self.images_dir):\n",
        "            for city in sorted(os.listdir(self.images_dir)):\n",
        "                img_dir_path = os.path.join(self.images_dir, city)\n",
        "                mask_dir_path = os.path.join(self.masks_dir, city)\n",
        "                if not os.path.isdir(img_dir_path): continue\n",
        "                for file_name in sorted(os.listdir(img_dir_path)):\n",
        "                    if file_name.endswith('_leftImg8bit.png'):\n",
        "                        self.images.append(os.path.join(img_dir_path, file_name))\n",
        "                        mask_name = file_name.replace('_leftImg8bit.png', '_gtFine_labelTrainIds.png')\n",
        "                        self.masks.append(os.path.join(mask_dir_path, mask_name))\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB').resize((1024, 512), Image.BILINEAR)\n",
        "        mask = Image.open(self.masks[idx]).resize((1024, 512), Image.NEAREST)\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image, torch.from_numpy(np.array(mask)).long()\n",
        "\n",
        "# --- EVALUATION ---\n",
        "print(f\"üöÄ detailed Evaluation for: {CHECKPOINT_PATH}\")\n",
        "\n",
        "model = BiSeNet(num_classes=NUM_CLASSES, context_path='resnet18')\n",
        "model.to(DEVICE)\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"‚úÖ Model loaded.\")\n",
        "else:\n",
        "    print(\"‚ùå Checkpoint not found.\")\n",
        "    exit()\n",
        "\n",
        "model.eval()\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "dataset = CityscapesDataset(CITYSCAPES_PATH, split='val', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "hist = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
        "\n",
        "print(\"Processing...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.numpy()\n",
        "        output = model(images)\n",
        "        if isinstance(output, tuple): output = output[0]\n",
        "        preds = torch.argmax(output, dim=1).cpu().numpy()\n",
        "\n",
        "        mask = (labels >= 0) & (labels < NUM_CLASSES)\n",
        "        hist += np.bincount(\n",
        "            NUM_CLASSES * labels[mask].astype(int) + preds[mask],\n",
        "            minlength=NUM_CLASSES ** 2\n",
        "        ).reshape(NUM_CLASSES, NUM_CLASSES)\n",
        "\n",
        "# --- CALCULATE & PRINT RESULTS ---\n",
        "iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "miou = np.nanmean(iou)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"üèÜ Final mIoU: {miou * 100:.2f}%\")\n",
        "print(\"=\"*40)\n",
        "print(\"üìù Per-Class IoU (Copy these to Table 4):\")\n",
        "print(\"-\" * 40)\n",
        "for i, class_name in enumerate(CLASSES):\n",
        "    print(f\"{class_name:15s}: {iou[i] * 100:.2f}%\")\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PCAWK5Hkdzq",
        "outputId": "d76668bb-5b6f-4734-a9f3-2d4b3f00d6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ detailed Evaluation for: /content/drive/MyDrive/bisenet_gta5_aug_checkpoint.pth\n",
            "‚úÖ Model loaded.\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:59<00:00,  2.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "üèÜ Final mIoU: 29.97%\n",
            "========================================\n",
            "üìù Per-Class IoU (Copy these to Table 4):\n",
            "----------------------------------------\n",
            "Road           : 71.81%\n",
            "Sidewalk       : 19.36%\n",
            "Building       : 74.84%\n",
            "Wall           : 26.07%\n",
            "Fence          : 14.29%\n",
            "Pole           : 24.28%\n",
            "Traffic Light  : 24.98%\n",
            "Traffic Sign   : 16.16%\n",
            "Vegetation     : 79.58%\n",
            "Terrain        : 23.30%\n",
            "Sky            : 78.03%\n",
            "Person         : 40.45%\n",
            "Rider          : 3.82%\n",
            "Car            : 50.56%\n",
            "Truck          : 5.90%\n",
            "Bus            : 3.24%\n",
            "Train          : 4.86%\n",
            "Motorcycle     : 6.70%\n",
            "Bicycle        : 1.17%\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}