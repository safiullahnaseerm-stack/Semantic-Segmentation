{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MOUNT DRIVE ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. RESTORE MODEL CODE ---\n",
        "if not os.path.exists('/content/models'):\n",
        "    !git clone https://github.com/Gabrysse/MLDL2024_project1.git temp_repo\n",
        "    shutil.copytree('temp_repo/models', '/content/models')\n",
        "    shutil.rmtree('temp_repo')\n",
        "    print(\"Models restored.\")\n",
        "\n",
        "# --- 3. RESTORE DATASET ---\n",
        "# Check if dataset exists, if not, unzip it\n",
        "if not os.path.exists('/content/dataset/project_data/gta5'):\n",
        "    print(\"Dataset not found. checking for zip file...\")\n",
        "\n",
        "    # Check paths (semseg folder first, then root)\n",
        "    zip_path_1 = '/content/drive/MyDrive/semseg/project_data.zip'\n",
        "    zip_path_2 = '/content/drive/MyDrive/project_data.zip'\n",
        "\n",
        "    if os.path.exists(zip_path_1):\n",
        "        print(f\"Unzipping from {zip_path_1}...\")\n",
        "        shutil.unpack_archive(zip_path_1, '/content/dataset')\n",
        "        print(\"Dataset extracted!\")\n",
        "    elif os.path.exists(zip_path_2):\n",
        "        print(f\"Unzipping from {zip_path_2}...\")\n",
        "        shutil.unpack_archive(zip_path_2, '/content/dataset')\n",
        "        print(\"Dataset extracted!\")\n",
        "    else:\n",
        "        print(\"Error: 'project_data.zip' not found in Drive. Please check your path.\")\n",
        "else:\n",
        "    print(\"Dataset is ready.\")\n",
        "\n",
        "# Add models to path\n",
        "sys.path.append('/content/models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gh66Gn4DRos",
        "outputId": "985e6694-8617-4c3f-fd20-8cec1c74b97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "Models restored.\n",
            "Dataset not found. checking for zip file...\n",
            "Unzipping from /content/drive/MyDrive/semseg/project_data.zip...\n",
            "Dataset extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.fft\n",
        "\n",
        "def extract_ampl_phase(fft_im):\n",
        "    # fft_im: size should be bx3xhx2w\n",
        "    fft_amp = fft_im[:,:,:,:,0]**2 + fft_im[:,:,:,:,1]**2\n",
        "    fft_amp = torch.sqrt(fft_amp)\n",
        "    fft_pha = torch.atan2(fft_im[:,:,:,:,1], fft_im[:,:,:,:,0])\n",
        "    return fft_amp, fft_pha\n",
        "\n",
        "def low_freq_mutate(amp_src, amp_trg, L=0.1):\n",
        "    # Expects 4D input: [Batch, Channel, Height, Width]\n",
        "    _, _, h, w = amp_src.size()\n",
        "    b = (np.floor(np.amin((h,w))*L)).astype(int)     # get b\n",
        "    amp_src[:,:,0:b,0:b]     = amp_trg[:,:,0:b,0:b]  # top left\n",
        "    amp_src[:,:,0:b,w-b:w]   = amp_trg[:,:,0:b,w-b:w] # top right\n",
        "    amp_src[:,:,h-b:h,0:b]   = amp_trg[:,:,h-b:h,0:b] # bottom left\n",
        "    amp_src[:,:,h-b:h,w-b:w] = amp_trg[:,:,h-b:h,w-b:w] # bottom right\n",
        "    return amp_src\n",
        "\n",
        "def FDA_source_to_target(src_img, trg_img, L=0.1):\n",
        "    # 1. Add a fake batch dimension to support 3D inputs [C,H,W] -> [1,C,H,W]\n",
        "    src_img = src_img.unsqueeze(0)\n",
        "    trg_img = trg_img.unsqueeze(0)\n",
        "\n",
        "    # get fft of both source and target\n",
        "    fft_src = torch.fft.rfft2(src_img.clone())\n",
        "    fft_trg = torch.fft.rfft2(trg_img.clone())\n",
        "\n",
        "    # extract amplitude and phase of both ffts\n",
        "    amp_src, pha_src = torch.abs(fft_src), torch.angle(fft_src)\n",
        "    amp_trg, pha_trg = torch.abs(fft_trg), torch.angle(fft_trg)\n",
        "\n",
        "    # mutate the amplitude part of source with target\n",
        "    amp_src_ = low_freq_mutate(amp_src.clone(), amp_trg.clone(), L=L)\n",
        "\n",
        "    # mutated fft of source\n",
        "    fft_src_ = torch.polar(amp_src_, pha_src)\n",
        "\n",
        "    # get the mutated image\n",
        "    src_in_trg = torch.fft.irfft2(fft_src_, s=src_img.shape[-2:])\n",
        "\n",
        "    # 2. Remove the fake batch dimension to return [C,H,W]\n",
        "    src_in_trg = src_in_trg.squeeze(0)\n",
        "\n",
        "    return src_in_trg"
      ],
      "metadata": {
        "id": "sPHZLF9ZFV4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "CHECKPOINT_NAME = 'bisenet_fda_checkpoint.pth'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 8\n",
        "L_BETA = 0.05  # Controls how much style is swapped (0.01 - 0.09 is standard)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Paths\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "SAVE_PATH = f'/content/drive/MyDrive/semseg/{CHECKPOINT_NAME}'\n",
        "\n",
        "# --- DATASET WITH FDA ---\n",
        "class GTA5_FDA_Dataset(Dataset):\n",
        "    def __init__(self, gta_root, city_root):\n",
        "        self.gta_images_dir = os.path.join(gta_root, 'images')\n",
        "        self.gta_masks_dir = os.path.join(gta_root, 'labels')\n",
        "        self.gta_images = sorted(os.listdir(self.gta_images_dir))\n",
        "\n",
        "        # Load Cityscapes images list (to steal style from)\n",
        "        self.city_images_dir = os.path.join(city_root, 'leftImg8bit', 'train')\n",
        "        self.city_images = []\n",
        "        if os.path.exists(self.city_images_dir):\n",
        "            for city in os.listdir(self.city_images_dir):\n",
        "                c_path = os.path.join(self.city_images_dir, city)\n",
        "                if os.path.isdir(c_path):\n",
        "                    for f in os.listdir(c_path):\n",
        "                        if f.endswith('_leftImg8bit.png'):\n",
        "                            self.city_images.append(os.path.join(c_path, f))\n",
        "\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.id_mapping = {\n",
        "            7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "            19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "            26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "        }\n",
        "\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self): return len(self.gta_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. Load GTA Image & Mask\n",
        "        img_path = os.path.join(self.gta_images_dir, self.gta_images[idx])\n",
        "        mask_path = os.path.join(self.gta_masks_dir, self.gta_images[idx])\n",
        "\n",
        "        gta_image = Image.open(img_path).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "        gta_mask = Image.open(mask_path).resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        # 2. Load Random Cityscapes Image (Target Style)\n",
        "        rand_idx = random.randint(0, len(self.city_images) - 1)\n",
        "        city_image = Image.open(self.city_images[rand_idx]).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "\n",
        "        # 3. Convert to Tensor\n",
        "        gta_t = self.to_tensor(gta_image)\n",
        "        city_t = self.to_tensor(city_image)\n",
        "\n",
        "        # 4. Apply FDA (Style Transfer)\n",
        "        # We wrap in try/except just in case of odd dimension issues, but the fix should handle it\n",
        "        try:\n",
        "            gta_stylized = FDA_source_to_target(gta_t, city_t, L=L_BETA)\n",
        "        except Exception as e:\n",
        "            print(f\"FDA Error: {e}\")\n",
        "            gta_stylized = gta_t # Fallback to original if FDA fails\n",
        "\n",
        "        # 5. Normalize\n",
        "        gta_stylized = self.normalize(gta_stylized)\n",
        "\n",
        "        # 6. Process Mask\n",
        "        mask_np = np.array(gta_mask)\n",
        "        target_mask = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in self.id_mapping.items(): target_mask[mask_np == k] = v\n",
        "\n",
        "        return gta_stylized, torch.from_numpy(target_mask).long()\n",
        "\n",
        "# --- TRAINING LOOP ---\n",
        "print(f\"Starting FDA Training (Beta={L_BETA})...\")\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "dataset = GTA5_FDA_Dataset(GTA_PATH, CITYSCAPES_PATH)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(\"Resuming from checkpoint...\")\n",
        "    checkpoint = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    for i, (img, lbl) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img.to(DEVICE))\n",
        "        loss = criterion(out[0], lbl.to(DEVICE)) + 0.1 * criterion(out[1], lbl.to(DEVICE)) + 0.1 * criterion(out[2], lbl.to(DEVICE))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Step [{i}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Save every epoch\n",
        "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, SAVE_PATH)\n",
        "    print(f\"Epoch {epoch+1} Saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyEI-h2CFnkd",
        "outputId": "6e82d4d9-f623-4ed7-96c3-77dff5180581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FDA Training (Beta=0.05)...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 122MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:01<00:00, 90.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Step [0/313] Loss: 4.0714\n",
            "Epoch [1/50] Step [50/313] Loss: 0.9045\n",
            "Epoch [1/50] Step [100/313] Loss: 0.8016\n",
            "Epoch [1/50] Step [150/313] Loss: 0.6505\n",
            "Epoch [1/50] Step [200/313] Loss: 0.6071\n",
            "Epoch [1/50] Step [250/313] Loss: 0.6166\n",
            "Epoch [1/50] Step [300/313] Loss: 0.5493\n",
            "Epoch 1 Saved.\n",
            "Epoch [2/50] Step [0/313] Loss: 0.5026\n",
            "Epoch [2/50] Step [50/313] Loss: 0.5384\n",
            "Epoch [2/50] Step [100/313] Loss: 0.4582\n",
            "Epoch [2/50] Step [150/313] Loss: 0.4515\n",
            "Epoch [2/50] Step [200/313] Loss: 0.4586\n",
            "Epoch [2/50] Step [250/313] Loss: 0.3605\n",
            "Epoch [2/50] Step [300/313] Loss: 0.3965\n",
            "Epoch 2 Saved.\n",
            "Epoch [3/50] Step [0/313] Loss: 0.4182\n",
            "Epoch [3/50] Step [50/313] Loss: 0.4937\n",
            "Epoch [3/50] Step [100/313] Loss: 0.3657\n",
            "Epoch [3/50] Step [150/313] Loss: 0.3702\n",
            "Epoch [3/50] Step [200/313] Loss: 0.3456\n",
            "Epoch [3/50] Step [250/313] Loss: 0.4183\n",
            "Epoch [3/50] Step [300/313] Loss: 0.3498\n",
            "Epoch 3 Saved.\n",
            "Epoch [4/50] Step [0/313] Loss: 0.3558\n",
            "Epoch [4/50] Step [50/313] Loss: 0.3469\n",
            "Epoch [4/50] Step [100/313] Loss: 0.4194\n",
            "Epoch [4/50] Step [150/313] Loss: 0.2939\n",
            "Epoch [4/50] Step [200/313] Loss: 0.3203\n",
            "Epoch [4/50] Step [250/313] Loss: 0.3701\n",
            "Epoch [4/50] Step [300/313] Loss: 0.3177\n",
            "Epoch 4 Saved.\n",
            "Epoch [5/50] Step [0/313] Loss: 0.3782\n",
            "Epoch [5/50] Step [50/313] Loss: 0.3507\n",
            "Epoch [5/50] Step [100/313] Loss: 0.4117\n",
            "Epoch [5/50] Step [150/313] Loss: 0.3902\n",
            "Epoch [5/50] Step [200/313] Loss: 0.2805\n",
            "Epoch [5/50] Step [250/313] Loss: 0.3444\n",
            "Epoch [5/50] Step [300/313] Loss: 0.3722\n",
            "Epoch 5 Saved.\n",
            "Epoch [6/50] Step [0/313] Loss: 0.3145\n",
            "Epoch [6/50] Step [50/313] Loss: 0.2905\n",
            "Epoch [6/50] Step [100/313] Loss: 0.3167\n",
            "Epoch [6/50] Step [150/313] Loss: 0.2515\n",
            "Epoch [6/50] Step [200/313] Loss: 0.2639\n",
            "Epoch [6/50] Step [250/313] Loss: 0.3141\n",
            "Epoch [6/50] Step [300/313] Loss: 0.2707\n",
            "Epoch 6 Saved.\n",
            "Epoch [7/50] Step [0/313] Loss: 0.2608\n",
            "Epoch [7/50] Step [50/313] Loss: 0.2921\n",
            "Epoch [7/50] Step [100/313] Loss: 0.2518\n",
            "Epoch [7/50] Step [150/313] Loss: 0.2708\n",
            "Epoch [7/50] Step [200/313] Loss: 0.2842\n",
            "Epoch [7/50] Step [250/313] Loss: 0.3005\n",
            "Epoch [7/50] Step [300/313] Loss: 0.3215\n",
            "Epoch 7 Saved.\n",
            "Epoch [8/50] Step [0/313] Loss: 0.3439\n",
            "Epoch [8/50] Step [50/313] Loss: 0.2864\n",
            "Epoch [8/50] Step [100/313] Loss: 0.2762\n",
            "Epoch [8/50] Step [150/313] Loss: 0.2853\n",
            "Epoch [8/50] Step [200/313] Loss: 0.2817\n",
            "Epoch [8/50] Step [250/313] Loss: 0.2146\n",
            "Epoch [8/50] Step [300/313] Loss: 0.2876\n",
            "Epoch 8 Saved.\n",
            "Epoch [9/50] Step [0/313] Loss: 0.2714\n",
            "Epoch [9/50] Step [50/313] Loss: 0.2514\n",
            "Epoch [9/50] Step [100/313] Loss: 0.2356\n",
            "Epoch [9/50] Step [150/313] Loss: 0.2714\n",
            "Epoch [9/50] Step [200/313] Loss: 0.2767\n",
            "Epoch [9/50] Step [250/313] Loss: 0.2378\n",
            "Epoch [9/50] Step [300/313] Loss: 0.2768\n",
            "Epoch 9 Saved.\n",
            "Epoch [10/50] Step [0/313] Loss: 0.2302\n",
            "Epoch [10/50] Step [50/313] Loss: 0.2716\n",
            "Epoch [10/50] Step [100/313] Loss: 0.2469\n",
            "Epoch [10/50] Step [150/313] Loss: 0.2250\n",
            "Epoch [10/50] Step [200/313] Loss: 0.2335\n",
            "Epoch [10/50] Step [250/313] Loss: 0.2466\n",
            "Epoch [10/50] Step [300/313] Loss: 0.2704\n",
            "Epoch 10 Saved.\n",
            "Epoch [11/50] Step [0/313] Loss: 0.2904\n",
            "Epoch [11/50] Step [50/313] Loss: 0.2514\n",
            "Epoch [11/50] Step [100/313] Loss: 0.2676\n",
            "Epoch [11/50] Step [150/313] Loss: 0.2247\n",
            "Epoch [11/50] Step [200/313] Loss: 0.2296\n",
            "Epoch [11/50] Step [250/313] Loss: 0.2378\n",
            "Epoch [11/50] Step [300/313] Loss: 0.2780\n",
            "Epoch 11 Saved.\n",
            "Epoch [12/50] Step [0/313] Loss: 0.2253\n",
            "Epoch [12/50] Step [50/313] Loss: 0.2371\n",
            "Epoch [12/50] Step [100/313] Loss: 0.2446\n",
            "Epoch [12/50] Step [150/313] Loss: 0.2110\n",
            "Epoch [12/50] Step [200/313] Loss: 0.2095\n",
            "Epoch [12/50] Step [250/313] Loss: 0.2292\n",
            "Epoch [12/50] Step [300/313] Loss: 0.2369\n",
            "Epoch 12 Saved.\n",
            "Epoch [13/50] Step [0/313] Loss: 0.2721\n",
            "Epoch [13/50] Step [50/313] Loss: 0.2594\n",
            "Epoch [13/50] Step [100/313] Loss: 0.2143\n",
            "Epoch [13/50] Step [150/313] Loss: 0.2476\n",
            "Epoch [13/50] Step [200/313] Loss: 0.2042\n",
            "Epoch [13/50] Step [250/313] Loss: 0.2089\n",
            "Epoch [13/50] Step [300/313] Loss: 0.2252\n",
            "Epoch 13 Saved.\n",
            "Epoch [14/50] Step [0/313] Loss: 0.2015\n",
            "Epoch [14/50] Step [50/313] Loss: 0.2068\n",
            "Epoch [14/50] Step [100/313] Loss: 0.1913\n",
            "Epoch [14/50] Step [150/313] Loss: 0.2027\n",
            "Epoch [14/50] Step [200/313] Loss: 0.1964\n",
            "Epoch [14/50] Step [250/313] Loss: 0.2354\n",
            "Epoch [14/50] Step [300/313] Loss: 0.2860\n",
            "Epoch 14 Saved.\n",
            "Epoch [15/50] Step [0/313] Loss: 0.2227\n",
            "Epoch [15/50] Step [50/313] Loss: 0.2137\n",
            "Epoch [15/50] Step [100/313] Loss: 0.2309\n",
            "Epoch [15/50] Step [150/313] Loss: 0.2250\n",
            "Epoch [15/50] Step [200/313] Loss: 0.2696\n",
            "Epoch [15/50] Step [250/313] Loss: 0.2583\n",
            "Epoch [15/50] Step [300/313] Loss: 0.1683\n",
            "Epoch 15 Saved.\n",
            "Epoch [16/50] Step [0/313] Loss: 0.2378\n",
            "Epoch [16/50] Step [50/313] Loss: 0.2075\n",
            "Epoch [16/50] Step [100/313] Loss: 0.2077\n",
            "Epoch [16/50] Step [150/313] Loss: 0.2120\n",
            "Epoch [16/50] Step [200/313] Loss: 0.2462\n",
            "Epoch [16/50] Step [250/313] Loss: 0.2217\n",
            "Epoch [16/50] Step [300/313] Loss: 0.1918\n",
            "Epoch 16 Saved.\n",
            "Epoch [17/50] Step [0/313] Loss: 0.2117\n",
            "Epoch [17/50] Step [50/313] Loss: 0.2083\n",
            "Epoch [17/50] Step [100/313] Loss: 0.1901\n",
            "Epoch [17/50] Step [150/313] Loss: 0.2278\n",
            "Epoch [17/50] Step [200/313] Loss: 0.1973\n",
            "Epoch [17/50] Step [250/313] Loss: 0.1849\n",
            "Epoch [17/50] Step [300/313] Loss: 0.2172\n",
            "Epoch 17 Saved.\n",
            "Epoch [18/50] Step [0/313] Loss: 0.2144\n",
            "Epoch [18/50] Step [50/313] Loss: 0.1860\n",
            "Epoch [18/50] Step [100/313] Loss: 0.2044\n",
            "Epoch [18/50] Step [150/313] Loss: 0.2635\n",
            "Epoch [18/50] Step [200/313] Loss: 0.2489\n",
            "Epoch [18/50] Step [250/313] Loss: 0.2527\n",
            "Epoch [18/50] Step [300/313] Loss: 0.2437\n",
            "Epoch 18 Saved.\n",
            "Epoch [19/50] Step [0/313] Loss: 0.1928\n",
            "Epoch [19/50] Step [50/313] Loss: 0.1837\n",
            "Epoch [19/50] Step [100/313] Loss: 0.2193\n",
            "Epoch [19/50] Step [150/313] Loss: 0.1759\n",
            "Epoch [19/50] Step [200/313] Loss: 0.1948\n",
            "Epoch [19/50] Step [250/313] Loss: 0.1811\n",
            "Epoch [19/50] Step [300/313] Loss: 0.2144\n",
            "Epoch 19 Saved.\n",
            "Epoch [20/50] Step [0/313] Loss: 0.1824\n",
            "Epoch [20/50] Step [50/313] Loss: 0.2032\n",
            "Epoch [20/50] Step [100/313] Loss: 0.2168\n",
            "Epoch [20/50] Step [150/313] Loss: 0.1904\n",
            "Epoch [20/50] Step [200/313] Loss: 0.2261\n",
            "Epoch [20/50] Step [250/313] Loss: 0.1979\n",
            "Epoch [20/50] Step [300/313] Loss: 0.2062\n",
            "Epoch 20 Saved.\n",
            "Epoch [21/50] Step [0/313] Loss: 0.1720\n",
            "Epoch [21/50] Step [50/313] Loss: 0.1745\n",
            "Epoch [21/50] Step [100/313] Loss: 0.1736\n",
            "Epoch [21/50] Step [150/313] Loss: 0.1954\n",
            "Epoch [21/50] Step [200/313] Loss: 0.1977\n",
            "Epoch [21/50] Step [250/313] Loss: 0.1875\n",
            "Epoch [21/50] Step [300/313] Loss: 0.1716\n",
            "Epoch 21 Saved.\n",
            "Epoch [22/50] Step [0/313] Loss: 0.2048\n",
            "Epoch [22/50] Step [50/313] Loss: 0.2245\n",
            "Epoch [22/50] Step [100/313] Loss: 0.1791\n",
            "Epoch [22/50] Step [150/313] Loss: 0.1808\n",
            "Epoch [22/50] Step [200/313] Loss: 0.1966\n",
            "Epoch [22/50] Step [250/313] Loss: 0.1449\n",
            "Epoch [22/50] Step [300/313] Loss: 0.1930\n",
            "Epoch 22 Saved.\n",
            "Epoch [23/50] Step [0/313] Loss: 0.1862\n",
            "Epoch [23/50] Step [50/313] Loss: 0.2051\n",
            "Epoch [23/50] Step [100/313] Loss: 0.2039\n",
            "Epoch [23/50] Step [150/313] Loss: 0.1948\n",
            "Epoch [23/50] Step [200/313] Loss: 0.2040\n",
            "Epoch [23/50] Step [250/313] Loss: 0.1736\n",
            "Epoch [23/50] Step [300/313] Loss: 0.1635\n",
            "Epoch 23 Saved.\n",
            "Epoch [24/50] Step [0/313] Loss: 0.1863\n",
            "Epoch [24/50] Step [50/313] Loss: 0.2151\n",
            "Epoch [24/50] Step [100/313] Loss: 0.1993\n",
            "Epoch [24/50] Step [150/313] Loss: 0.2035\n",
            "Epoch [24/50] Step [200/313] Loss: 0.2213\n",
            "Epoch [24/50] Step [250/313] Loss: 0.2045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# Configuration\n",
        "CHECKPOINT_NAME = 'bisenet_fda_checkpoint.pth'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 8\n",
        "L_BETA = 0.05\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "SAVE_PATH = f'/content/drive/MyDrive/semseg/{CHECKPOINT_NAME}'\n",
        "\n",
        "class GTA5_FDA_Dataset(Dataset):\n",
        "    def __init__(self, gta_root, city_root):\n",
        "        self.gta_images_dir = os.path.join(gta_root, 'images')\n",
        "        self.gta_masks_dir = os.path.join(gta_root, 'labels')\n",
        "        self.gta_images = sorted(os.listdir(self.gta_images_dir))\n",
        "\n",
        "        self.city_images_dir = os.path.join(city_root, 'leftImg8bit', 'train')\n",
        "        self.city_images = []\n",
        "        if os.path.exists(self.city_images_dir):\n",
        "            for city in os.listdir(self.city_images_dir):\n",
        "                c_path = os.path.join(self.city_images_dir, city)\n",
        "                if os.path.isdir(c_path):\n",
        "                    for f in os.listdir(c_path):\n",
        "                        if f.endswith('_leftImg8bit.png'):\n",
        "                            self.city_images.append(os.path.join(c_path, f))\n",
        "\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.id_mapping = {\n",
        "            7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "            19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "            26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "        }\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self): return len(self.gta_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.gta_images_dir, self.gta_images[idx])\n",
        "        mask_path = os.path.join(self.gta_masks_dir, self.gta_images[idx])\n",
        "\n",
        "        gta_image = Image.open(img_path).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "        gta_mask = Image.open(mask_path).resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        rand_idx = random.randint(0, len(self.city_images) - 1)\n",
        "        city_image = Image.open(self.city_images[rand_idx]).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "\n",
        "        gta_t = self.to_tensor(gta_image)\n",
        "        city_t = self.to_tensor(city_image)\n",
        "\n",
        "        try:\n",
        "            gta_stylized = FDA_source_to_target(gta_t, city_t, L=L_BETA)\n",
        "        except Exception as e:\n",
        "            print(f\"FDA Error: {e}\")\n",
        "            gta_stylized = gta_t\n",
        "\n",
        "        gta_stylized = self.normalize(gta_stylized)\n",
        "        mask_np = np.array(gta_mask)\n",
        "        target_mask = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in self.id_mapping.items(): target_mask[mask_np == k] = v\n",
        "        return gta_stylized, torch.from_numpy(target_mask).long()\n",
        "\n",
        "print(f\"Resuming FDA Training (Beta={L_BETA})...\")\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(\"Loading Checkpoint...\")\n",
        "    checkpoint = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "    print(f\"Starting from Epoch {start_epoch + 1}\")\n",
        "else:\n",
        "    print(\"No checkpoint found! Starting from Epoch 1\")\n",
        "    start_epoch = 0\n",
        "\n",
        "dataset = GTA5_FDA_Dataset(GTA_PATH, CITYSCAPES_PATH)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    for i, (img, lbl) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img.to(DEVICE))\n",
        "        loss = criterion(out[0], lbl.to(DEVICE)) + 0.1 * criterion(out[1], lbl.to(DEVICE)) + 0.1 * criterion(out[2], lbl.to(DEVICE))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Step [{i}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, SAVE_PATH)\n",
        "    print(f\"Epoch {epoch+1} Saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL50HYVfC4EB",
        "outputId": "95c46779-7b51-4c12-f73c-c52afc94da5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming FDA Training (Beta=0.05)...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 200MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:01<00:00, 126MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Checkpoint...\n",
            "Starting from Epoch 25\n",
            "Epoch [25/50] Step [0/313] Loss: 0.1825\n",
            "Epoch [25/50] Step [50/313] Loss: 0.2239\n",
            "Epoch [25/50] Step [100/313] Loss: 0.2482\n",
            "Epoch [25/50] Step [150/313] Loss: 0.1816\n",
            "Epoch [25/50] Step [200/313] Loss: 0.2578\n",
            "Epoch [25/50] Step [250/313] Loss: 0.2024\n",
            "Epoch [25/50] Step [300/313] Loss: 0.1912\n",
            "Epoch 25 Saved.\n",
            "Epoch [26/50] Step [0/313] Loss: 0.1703\n",
            "Epoch [26/50] Step [50/313] Loss: 0.1905\n",
            "Epoch [26/50] Step [100/313] Loss: 0.1911\n",
            "Epoch [26/50] Step [150/313] Loss: 0.1717\n",
            "Epoch [26/50] Step [200/313] Loss: 0.1811\n",
            "Epoch [26/50] Step [250/313] Loss: 0.2127\n",
            "Epoch [26/50] Step [300/313] Loss: 0.1817\n",
            "Epoch 26 Saved.\n",
            "Epoch [27/50] Step [0/313] Loss: 0.1939\n",
            "Epoch [27/50] Step [50/313] Loss: 0.1629\n",
            "Epoch [27/50] Step [100/313] Loss: 0.1733\n",
            "Epoch [27/50] Step [150/313] Loss: 0.1810\n",
            "Epoch [27/50] Step [200/313] Loss: 0.1659\n",
            "Epoch [27/50] Step [250/313] Loss: 0.1810\n",
            "Epoch [27/50] Step [300/313] Loss: 0.1686\n",
            "Epoch 27 Saved.\n",
            "Epoch [28/50] Step [0/313] Loss: 0.1807\n",
            "Epoch [28/50] Step [50/313] Loss: 0.1651\n",
            "Epoch [28/50] Step [100/313] Loss: 0.1660\n",
            "Epoch [28/50] Step [150/313] Loss: 0.2055\n",
            "Epoch [28/50] Step [200/313] Loss: 0.1673\n",
            "Epoch [28/50] Step [250/313] Loss: 0.1725\n",
            "Epoch [28/50] Step [300/313] Loss: 0.1650\n",
            "Epoch 28 Saved.\n",
            "Epoch [29/50] Step [0/313] Loss: 0.1645\n",
            "Epoch [29/50] Step [50/313] Loss: 0.1872\n",
            "Epoch [29/50] Step [100/313] Loss: 0.1858\n",
            "Epoch [29/50] Step [150/313] Loss: 0.1945\n",
            "Epoch [29/50] Step [200/313] Loss: 0.1581\n",
            "Epoch [29/50] Step [250/313] Loss: 0.1854\n",
            "Epoch [29/50] Step [300/313] Loss: 0.1552\n",
            "Epoch 29 Saved.\n",
            "Epoch [30/50] Step [0/313] Loss: 0.1604\n",
            "Epoch [30/50] Step [50/313] Loss: 0.1627\n",
            "Epoch [30/50] Step [100/313] Loss: 0.1725\n",
            "Epoch [30/50] Step [150/313] Loss: 0.1857\n",
            "Epoch [30/50] Step [200/313] Loss: 0.1424\n",
            "Epoch [30/50] Step [250/313] Loss: 0.1596\n",
            "Epoch [30/50] Step [300/313] Loss: 0.2038\n",
            "Epoch 30 Saved.\n",
            "Epoch [31/50] Step [0/313] Loss: 0.1702\n",
            "Epoch [31/50] Step [50/313] Loss: 0.1567\n",
            "Epoch [31/50] Step [100/313] Loss: 0.1704\n",
            "Epoch [31/50] Step [150/313] Loss: 0.2108\n",
            "Epoch [31/50] Step [200/313] Loss: 0.1753\n",
            "Epoch [31/50] Step [250/313] Loss: 0.1769\n",
            "Epoch [31/50] Step [300/313] Loss: 0.1895\n",
            "Epoch 31 Saved.\n",
            "Epoch [32/50] Step [0/313] Loss: 0.1603\n",
            "Epoch [32/50] Step [50/313] Loss: 0.1643\n",
            "Epoch [32/50] Step [100/313] Loss: 0.1882\n",
            "Epoch [32/50] Step [150/313] Loss: 0.1491\n",
            "Epoch [32/50] Step [200/313] Loss: 0.1450\n",
            "Epoch [32/50] Step [250/313] Loss: 0.1576\n",
            "Epoch [32/50] Step [300/313] Loss: 0.1705\n",
            "Epoch 32 Saved.\n",
            "Epoch [33/50] Step [0/313] Loss: 0.1980\n",
            "Epoch [33/50] Step [50/313] Loss: 0.1771\n",
            "Epoch [33/50] Step [100/313] Loss: 0.1750\n",
            "Epoch [33/50] Step [150/313] Loss: 0.1690\n",
            "Epoch [33/50] Step [200/313] Loss: 0.1540\n",
            "Epoch [33/50] Step [250/313] Loss: 0.1541\n",
            "Epoch [33/50] Step [300/313] Loss: 0.1509\n",
            "Epoch 33 Saved.\n",
            "Epoch [34/50] Step [0/313] Loss: 0.1870\n",
            "Epoch [34/50] Step [50/313] Loss: 0.1619\n",
            "Epoch [34/50] Step [100/313] Loss: 0.1886\n",
            "Epoch [34/50] Step [150/313] Loss: 0.1804\n",
            "Epoch [34/50] Step [200/313] Loss: 0.1693\n",
            "Epoch [34/50] Step [250/313] Loss: 0.1249\n",
            "Epoch [34/50] Step [300/313] Loss: 0.1668\n",
            "Epoch 34 Saved.\n",
            "Epoch [35/50] Step [0/313] Loss: 0.1985\n",
            "Epoch [35/50] Step [50/313] Loss: 0.1830\n",
            "Epoch [35/50] Step [100/313] Loss: 0.1838\n",
            "Epoch [35/50] Step [150/313] Loss: 0.1660\n",
            "Epoch [35/50] Step [200/313] Loss: 0.1508\n",
            "Epoch [35/50] Step [250/313] Loss: 0.1840\n",
            "Epoch [35/50] Step [300/313] Loss: 0.1656\n",
            "Epoch 35 Saved.\n",
            "Epoch [36/50] Step [0/313] Loss: 0.1840\n",
            "Epoch [36/50] Step [50/313] Loss: 0.1644\n",
            "Epoch [36/50] Step [100/313] Loss: 0.2057\n",
            "Epoch [36/50] Step [150/313] Loss: 0.1666\n",
            "Epoch [36/50] Step [200/313] Loss: 0.2084\n",
            "Epoch [36/50] Step [250/313] Loss: 0.1481\n",
            "Epoch [36/50] Step [300/313] Loss: 0.1921\n",
            "Epoch 36 Saved.\n",
            "Epoch [37/50] Step [0/313] Loss: 0.1480\n",
            "Epoch [37/50] Step [50/313] Loss: 0.1402\n",
            "Epoch [37/50] Step [100/313] Loss: 0.1477\n",
            "Epoch [37/50] Step [150/313] Loss: 0.1581\n",
            "Epoch [37/50] Step [200/313] Loss: 0.1687\n",
            "Epoch [37/50] Step [250/313] Loss: 0.1717\n",
            "Epoch [37/50] Step [300/313] Loss: 0.1721\n",
            "Epoch 37 Saved.\n",
            "Epoch [38/50] Step [0/313] Loss: 0.1684\n",
            "Epoch [38/50] Step [50/313] Loss: 0.1597\n",
            "Epoch [38/50] Step [100/313] Loss: 0.1677\n",
            "Epoch [38/50] Step [150/313] Loss: 0.1794\n",
            "Epoch [38/50] Step [200/313] Loss: 0.1831\n",
            "Epoch [38/50] Step [250/313] Loss: 0.1252\n",
            "Epoch [38/50] Step [300/313] Loss: 0.1727\n",
            "Epoch 38 Saved.\n",
            "Epoch [39/50] Step [0/313] Loss: 0.1699\n",
            "Epoch [39/50] Step [50/313] Loss: 0.1593\n",
            "Epoch [39/50] Step [100/313] Loss: 0.1600\n",
            "Epoch [39/50] Step [150/313] Loss: 0.1729\n",
            "Epoch [39/50] Step [200/313] Loss: 0.1486\n",
            "Epoch [39/50] Step [250/313] Loss: 0.1859\n",
            "Epoch [39/50] Step [300/313] Loss: 0.1673\n",
            "Epoch 39 Saved.\n",
            "Epoch [40/50] Step [0/313] Loss: 0.1903\n",
            "Epoch [40/50] Step [50/313] Loss: 0.1473\n",
            "Epoch [40/50] Step [100/313] Loss: 0.1706\n",
            "Epoch [40/50] Step [150/313] Loss: 0.2805\n",
            "Epoch [40/50] Step [200/313] Loss: 0.2273\n",
            "Epoch [40/50] Step [250/313] Loss: 0.2145\n",
            "Epoch [40/50] Step [300/313] Loss: 0.1906\n",
            "Epoch 40 Saved.\n",
            "Epoch [41/50] Step [0/313] Loss: 0.1581\n",
            "Epoch [41/50] Step [50/313] Loss: 0.2259\n",
            "Epoch [41/50] Step [100/313] Loss: 0.1783\n",
            "Epoch [41/50] Step [150/313] Loss: 0.2074\n",
            "Epoch [41/50] Step [200/313] Loss: 0.1966\n",
            "Epoch [41/50] Step [250/313] Loss: 0.1449\n",
            "Epoch [41/50] Step [300/313] Loss: 0.1568\n",
            "Epoch 41 Saved.\n",
            "Epoch [42/50] Step [0/313] Loss: 0.1888\n",
            "Epoch [42/50] Step [50/313] Loss: 0.1609\n",
            "Epoch [42/50] Step [100/313] Loss: 0.1673\n",
            "Epoch [42/50] Step [150/313] Loss: 0.1624\n",
            "Epoch [42/50] Step [200/313] Loss: 0.1647\n",
            "Epoch [42/50] Step [250/313] Loss: 0.1292\n",
            "Epoch [42/50] Step [300/313] Loss: 0.1673\n",
            "Epoch 42 Saved.\n",
            "Epoch [43/50] Step [0/313] Loss: 0.1482\n",
            "Epoch [43/50] Step [50/313] Loss: 0.1776\n",
            "Epoch [43/50] Step [100/313] Loss: 0.1590\n",
            "Epoch [43/50] Step [150/313] Loss: 0.1658\n",
            "Epoch [43/50] Step [200/313] Loss: 0.1938\n",
            "Epoch [43/50] Step [250/313] Loss: 0.1551\n",
            "Epoch [43/50] Step [300/313] Loss: 0.1918\n",
            "Epoch 43 Saved.\n",
            "Epoch [44/50] Step [0/313] Loss: 0.1647\n",
            "Epoch [44/50] Step [50/313] Loss: 0.1497\n",
            "Epoch [44/50] Step [100/313] Loss: 0.1771\n",
            "Epoch [44/50] Step [150/313] Loss: 0.1711\n",
            "Epoch [44/50] Step [200/313] Loss: 0.1530\n",
            "Epoch [44/50] Step [250/313] Loss: 0.1703\n",
            "Epoch [44/50] Step [300/313] Loss: 0.1390\n",
            "Epoch 44 Saved.\n",
            "Epoch [45/50] Step [0/313] Loss: 0.1677\n",
            "Epoch [45/50] Step [50/313] Loss: 0.1481\n",
            "Epoch [45/50] Step [100/313] Loss: 0.1457\n",
            "Epoch [45/50] Step [150/313] Loss: 0.1496\n",
            "Epoch [45/50] Step [200/313] Loss: 0.1556\n",
            "Epoch [45/50] Step [250/313] Loss: 0.1798\n",
            "Epoch [45/50] Step [300/313] Loss: 0.1699\n",
            "Epoch 45 Saved.\n",
            "Epoch [46/50] Step [0/313] Loss: 0.1366\n",
            "Epoch [46/50] Step [50/313] Loss: 0.1735\n",
            "Epoch [46/50] Step [100/313] Loss: 0.1482\n",
            "Epoch [46/50] Step [150/313] Loss: 0.1510\n",
            "Epoch [46/50] Step [200/313] Loss: 0.1492\n",
            "Epoch [46/50] Step [250/313] Loss: 0.1998\n",
            "Epoch [46/50] Step [300/313] Loss: 0.1874\n",
            "Epoch 46 Saved.\n",
            "Epoch [47/50] Step [0/313] Loss: 0.1813\n",
            "Epoch [47/50] Step [50/313] Loss: 0.1574\n",
            "Epoch [47/50] Step [100/313] Loss: 0.1457\n",
            "Epoch [47/50] Step [150/313] Loss: 0.1519\n",
            "Epoch [47/50] Step [200/313] Loss: 0.1612\n",
            "Epoch [47/50] Step [250/313] Loss: 0.1513\n",
            "Epoch [47/50] Step [300/313] Loss: 0.1499\n",
            "Epoch 47 Saved.\n",
            "Epoch [48/50] Step [0/313] Loss: 0.1597\n",
            "Epoch [48/50] Step [50/313] Loss: 0.1582\n",
            "Epoch [48/50] Step [100/313] Loss: 0.1391\n",
            "Epoch [48/50] Step [150/313] Loss: 0.1679\n",
            "Epoch [48/50] Step [200/313] Loss: 0.1778\n",
            "Epoch [48/50] Step [250/313] Loss: 0.1705\n",
            "Epoch [48/50] Step [300/313] Loss: 0.1308\n",
            "Epoch 48 Saved.\n",
            "Epoch [49/50] Step [0/313] Loss: 0.1625\n",
            "Epoch [49/50] Step [50/313] Loss: 0.1673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import random\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# Configuration\n",
        "CHECKPOINT_NAME = 'bisenet_fda_checkpoint.pth'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 8\n",
        "L_BETA = 0.05\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "SAVE_PATH = f'/content/drive/MyDrive/semseg/{CHECKPOINT_NAME}'\n",
        "\n",
        "class GTA5_FDA_Dataset(Dataset):\n",
        "    def __init__(self, gta_root, city_root):\n",
        "        self.gta_images_dir = os.path.join(gta_root, 'images')\n",
        "        self.gta_masks_dir = os.path.join(gta_root, 'labels')\n",
        "        self.gta_images = sorted(os.listdir(self.gta_images_dir))\n",
        "\n",
        "        self.city_images_dir = os.path.join(city_root, 'leftImg8bit', 'train')\n",
        "        self.city_images = []\n",
        "        if os.path.exists(self.city_images_dir):\n",
        "            for city in os.listdir(self.city_images_dir):\n",
        "                c_path = os.path.join(self.city_images_dir, city)\n",
        "                if os.path.isdir(c_path):\n",
        "                    for f in os.listdir(c_path):\n",
        "                        if f.endswith('_leftImg8bit.png'):\n",
        "                            self.city_images.append(os.path.join(c_path, f))\n",
        "\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.id_mapping = {\n",
        "            7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "            19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "            26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "        }\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self): return len(self.gta_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.gta_images_dir, self.gta_images[idx])\n",
        "        mask_path = os.path.join(self.gta_masks_dir, self.gta_images[idx])\n",
        "\n",
        "        gta_image = Image.open(img_path).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "        gta_mask = Image.open(mask_path).resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        rand_idx = random.randint(0, len(self.city_images) - 1)\n",
        "        city_image = Image.open(self.city_images[rand_idx]).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "\n",
        "        gta_t = self.to_tensor(gta_image)\n",
        "        city_t = self.to_tensor(city_image)\n",
        "\n",
        "        try:\n",
        "            gta_stylized = FDA_source_to_target(gta_t, city_t, L=L_BETA)\n",
        "        except Exception:\n",
        "            gta_stylized = gta_t\n",
        "\n",
        "        gta_stylized = self.normalize(gta_stylized)\n",
        "        mask_np = np.array(gta_mask)\n",
        "        target_mask = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in self.id_mapping.items(): target_mask[mask_np == k] = v\n",
        "        return gta_stylized, torch.from_numpy(target_mask).long()\n",
        "\n",
        "print(f\"Resuming FDA Training...\")\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(\"Found checkpoint. Loading...\")\n",
        "    checkpoint = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "    print(f\"Starting from Epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting from Epoch 0\")\n",
        "    start_epoch = 0\n",
        "\n",
        "dataset = GTA5_FDA_Dataset(GTA_PATH, CITYSCAPES_PATH)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    for i, (img, lbl) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img.to(DEVICE))\n",
        "        loss = criterion(out[0], lbl.to(DEVICE)) + 0.1 * criterion(out[1], lbl.to(DEVICE)) + 0.1 * criterion(out[2], lbl.to(DEVICE))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Step [{i}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, SAVE_PATH)\n",
        "    print(f\"Epoch {epoch+1} Saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyUnc0cAC6gZ",
        "outputId": "08c89a74-f355-41d9-fa66-23cad6b84b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming FDA Training...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 209MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:01<00:00, 169MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found checkpoint. Loading...\n",
            "Starting from Epoch 48\n",
            "Epoch [49/50] Step [0/313] Loss: 0.1600\n",
            "Epoch [49/50] Step [50/313] Loss: 0.1380\n",
            "Epoch [49/50] Step [100/313] Loss: 0.1621\n",
            "Epoch [49/50] Step [150/313] Loss: 0.1383\n",
            "Epoch [49/50] Step [200/313] Loss: 0.1651\n",
            "Epoch [49/50] Step [250/313] Loss: 0.1861\n",
            "Epoch [49/50] Step [300/313] Loss: 0.1486\n",
            "Epoch 49 Saved.\n",
            "Epoch [50/50] Step [0/313] Loss: 0.1524\n",
            "Epoch [50/50] Step [50/313] Loss: 0.1645\n",
            "Epoch [50/50] Step [100/313] Loss: 0.1374\n",
            "Epoch [50/50] Step [150/313] Loss: 0.1317\n",
            "Epoch [50/50] Step [200/313] Loss: 0.2614\n",
            "Epoch [50/50] Step [250/313] Loss: 0.1706\n",
            "Epoch [50/50] Step [300/313] Loss: 0.1393\n",
            "Epoch 50 Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Configuration\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/semseg/bisenet_fda_checkpoint.pth'\n",
        "NUM_CLASSES = 19\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "CLASSES = [\n",
        "    \"Road\", \"Sidewalk\", \"Building\", \"Wall\", \"Fence\", \"Pole\",\n",
        "    \"Traffic Light\", \"Traffic Sign\", \"Vegetation\", \"Terrain\", \"Sky\",\n",
        "    \"Person\", \"Rider\", \"Car\", \"Truck\", \"Bus\", \"Train\", \"Motorcycle\", \"Bicycle\"\n",
        "]\n",
        "\n",
        "if os.path.exists('/content/models'): sys.path.append('/content/models')\n",
        "\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root, split='val', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.images_dir = os.path.join(root, 'leftImg8bit', split)\n",
        "        self.masks_dir = os.path.join(root, 'gtFine', split)\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "        if os.path.exists(self.images_dir):\n",
        "            for city in sorted(os.listdir(self.images_dir)):\n",
        "                img_dir_path = os.path.join(self.images_dir, city)\n",
        "                mask_dir_path = os.path.join(self.masks_dir, city)\n",
        "                if not os.path.isdir(img_dir_path): continue\n",
        "                for file_name in sorted(os.listdir(img_dir_path)):\n",
        "                    if file_name.endswith('_leftImg8bit.png'):\n",
        "                        self.images.append(os.path.join(img_dir_path, file_name))\n",
        "                        mask_name = file_name.replace('_leftImg8bit.png', '_gtFine_labelTrainIds.png')\n",
        "                        self.masks.append(os.path.join(mask_dir_path, mask_name))\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB').resize((1024, 512), Image.BILINEAR)\n",
        "        mask = Image.open(self.masks[idx]).resize((1024, 512), Image.NEAREST)\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image, torch.from_numpy(np.array(mask)).long()\n",
        "\n",
        "print(f\"Evaluating: {CHECKPOINT_PATH}\")\n",
        "model = BiSeNet(num_classes=NUM_CLASSES, context_path='resnet18')\n",
        "model.to(DEVICE)\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Model loaded.\")\n",
        "else:\n",
        "    print(f\"Checkpoint not found at {CHECKPOINT_PATH}\")\n",
        "    sys.exit()\n",
        "\n",
        "model.eval()\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "dataset = CityscapesDataset(CITYSCAPES_PATH, split='val', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "hist = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
        "print(\"Processing validation images...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.numpy()\n",
        "        output = model(images)\n",
        "        if isinstance(output, tuple): output = output[0]\n",
        "        preds = torch.argmax(output, dim=1).cpu().numpy()\n",
        "        mask = (labels >= 0) & (labels < NUM_CLASSES)\n",
        "        hist += np.bincount(\n",
        "            NUM_CLASSES * labels[mask].astype(int) + preds[mask],\n",
        "            minlength=NUM_CLASSES ** 2\n",
        "        ).reshape(NUM_CLASSES, NUM_CLASSES)\n",
        "\n",
        "iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "miou = np.nanmean(iou)\n",
        "\n",
        "print(f\"\\nFinal mIoU (FDA): {miou * 100:.2f}%\")\n",
        "print(\"-\" * 30)\n",
        "for i, class_name in enumerate(CLASSES):\n",
        "    print(f\"{class_name:15s}: {iou[i] * 100:.2f}%\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueB2CR4wJdba",
        "outputId": "49470f15-1c25-40f4-ed86-0e0181b20e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating: /content/drive/MyDrive/semseg/bisenet_fda_checkpoint.pth\n",
            "Model loaded.\n",
            "Processing validation images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:56<00:00,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final mIoU (FDA): 26.98%\n",
            "------------------------------\n",
            "Road           : 71.53%\n",
            "Sidewalk       : 25.06%\n",
            "Building       : 67.71%\n",
            "Wall           : 15.36%\n",
            "Fence          : 12.14%\n",
            "Pole           : 21.71%\n",
            "Traffic Light  : 16.41%\n",
            "Traffic Sign   : 12.93%\n",
            "Vegetation     : 79.07%\n",
            "Terrain        : 17.43%\n",
            "Sky            : 70.83%\n",
            "Person         : 36.45%\n",
            "Rider          : 0.29%\n",
            "Car            : 47.13%\n",
            "Truck          : 8.06%\n",
            "Bus            : 0.47%\n",
            "Train          : 5.19%\n",
            "Motorcycle     : 4.78%\n",
            "Bicycle        : 0.02%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}