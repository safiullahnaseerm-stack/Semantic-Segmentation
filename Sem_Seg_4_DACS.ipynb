{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MOUNT DRIVE ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. RESTORE MODEL CODE ---\n",
        "if not os.path.exists('/content/models'):\n",
        "    !git clone https://github.com/Gabrysse/MLDL2024_project1.git temp_repo\n",
        "    shutil.copytree('temp_repo/models', '/content/models')\n",
        "    shutil.rmtree('temp_repo')\n",
        "    print(\"Models restored.\")\n",
        "\n",
        "# --- 3. RESTORE DATASET ---\n",
        "# Check if dataset exists, if not, unzip it\n",
        "if not os.path.exists('/content/dataset/project_data/gta5'):\n",
        "    print(\"Dataset not found. checking for zip file...\")\n",
        "\n",
        "    # Check paths (semseg folder first, then root)\n",
        "    zip_path_1 = '/content/drive/MyDrive/semseg/project_data.zip'\n",
        "    zip_path_2 = '/content/drive/MyDrive/project_data.zip'\n",
        "\n",
        "    if os.path.exists(zip_path_1):\n",
        "        print(f\"Unzipping from {zip_path_1}...\")\n",
        "        shutil.unpack_archive(zip_path_1, '/content/dataset')\n",
        "        print(\"Dataset extracted!\")\n",
        "    elif os.path.exists(zip_path_2):\n",
        "        print(f\"Unzipping from {zip_path_2}...\")\n",
        "        shutil.unpack_archive(zip_path_2, '/content/dataset')\n",
        "        print(\"Dataset extracted!\")\n",
        "    else:\n",
        "        print(\"Error: 'project_data.zip' not found in Drive. Please check your path.\")\n",
        "else:\n",
        "    print(\"Dataset is ready.\")\n",
        "\n",
        "# Add models to path\n",
        "sys.path.append('/content/models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gh66Gn4DRos",
        "outputId": "985e6694-8617-4c3f-fd20-8cec1c74b97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "Receiving objects: 100% (34/34), 11.29 KiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "remote: Total 34 (delta 9), reused 3 (delta 3), pack-reused 13 (from 1)\u001b[K\n",
            "Models restored.\n",
            "Dataset not found. checking for zip file...\n",
            "Unzipping from /content/drive/MyDrive/semseg/project_data.zip...\n",
            "Dataset extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "\n",
        "CHECKPOINT_NAME = 'bisenet_dacs_thresholded.pth'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 8\n",
        "LR = 2.5e-2\n",
        "CONFIDENCE_THRESHOLD = 0.968\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "SAVE_PATH = f'/content/drive/MyDrive/semseg/{CHECKPOINT_NAME}'\n",
        "\n",
        "# --- DATASET CLASS ---\n",
        "class GTA5_City_Dataset(Dataset):\n",
        "    def __init__(self, gta_root, city_root):\n",
        "        self.gta_images_dir = os.path.join(gta_root, 'images')\n",
        "        self.gta_masks_dir = os.path.join(gta_root, 'labels')\n",
        "        self.gta_images = sorted(os.listdir(self.gta_images_dir))\n",
        "\n",
        "        self.city_images_dir = os.path.join(city_root, 'leftImg8bit', 'train')\n",
        "        self.city_images = []\n",
        "        if os.path.exists(self.city_images_dir):\n",
        "            for city in os.listdir(self.city_images_dir):\n",
        "                c_path = os.path.join(self.city_images_dir, city)\n",
        "                if os.path.isdir(c_path):\n",
        "                    for f in os.listdir(c_path):\n",
        "                        if f.endswith('_leftImg8bit.png'):\n",
        "                            self.city_images.append(os.path.join(c_path, f))\n",
        "\n",
        "        # Color Jitter for Source (GTA) - Essential for Domain Adaptation\n",
        "        self.color_jitter = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.id_mapping = {\n",
        "            7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "            19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "            26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "        }\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self): return len(self.gta_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Source (GTA)\n",
        "        gta_path = os.path.join(self.gta_images_dir, self.gta_images[idx])\n",
        "        mask_path = os.path.join(self.gta_masks_dir, self.gta_images[idx])\n",
        "\n",
        "        gta_img = Image.open(gta_path).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "        gta_mask = Image.open(mask_path).resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        # Load Target (Cityscapes)\n",
        "        rand_idx = random.randint(0, len(self.city_images) - 1)\n",
        "        city_img = Image.open(self.city_images[rand_idx]).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "\n",
        "        # Apply Transforms\n",
        "        gta_img_aug = self.color_jitter(gta_img)\n",
        "        gta_t = self.normalize(self.to_tensor(gta_img_aug))\n",
        "        city_t = self.normalize(self.to_tensor(city_img))\n",
        "\n",
        "        # Process Mask\n",
        "        mask_np = np.array(gta_mask)\n",
        "        gta_lbl = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in self.id_mapping.items(): gta_lbl[mask_np == k] = v\n",
        "\n",
        "        return gta_t, torch.from_numpy(gta_lbl).long(), city_t\n",
        "\n",
        "# --- DACS MIXING FUNCTION ---\n",
        "def dacs_mix(gta_img, gta_lbl, city_img, city_pseudo_lbl):\n",
        "    \"\"\"\n",
        "    Mixes Source (GTA) classes onto Target (Cityscapes) images.\n",
        "    \"\"\"\n",
        "    mixed_img = city_img.clone()\n",
        "    mixed_lbl = city_pseudo_lbl.clone()\n",
        "\n",
        "    for i in range(gta_img.shape[0]):\n",
        "        classes_in_img = torch.unique(gta_lbl[i])\n",
        "        classes_in_img = classes_in_img[classes_in_img != 255]\n",
        "\n",
        "        if len(classes_in_img) > 0:\n",
        "            n_classes = len(classes_in_img)\n",
        "\n",
        "            perm = torch.randperm(n_classes)\n",
        "            selected_classes = classes_in_img[perm[:(n_classes + 1) // 2]]\n",
        "\n",
        "            mask = torch.zeros_like(gta_lbl[i]).bool()\n",
        "            for c in selected_classes:\n",
        "                mask = mask | (gta_lbl[i] == c)\n",
        "\n",
        "            mixed_img[i, :, mask] = gta_img[i, :, mask]\n",
        "            mixed_lbl[i, mask] = gta_lbl[i, mask]\n",
        "\n",
        "    return mixed_img, mixed_lbl\n",
        "\n",
        "# --- TRAINING SETUP ---\n",
        "print(\"Starting Advanced DACS Training (With Thresholding)...\")\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Poly Scheduler\n",
        "scheduler = optim.lr_scheduler.PolyLR(optimizer, total_iters=EPOCHS, power=0.9) if hasattr(optim.lr_scheduler, 'PolyLR') else optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "# Loss Function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "dataset = GTA5_City_Dataset(GTA_PATH, CITYSCAPES_PATH)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(\"Resuming from checkpoint...\")\n",
        "    checkpoint = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "# --- TRAINING LOOP ---\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    for i, (gta_img, gta_lbl, city_img) in enumerate(loader):\n",
        "        gta_img, gta_lbl, city_img = gta_img.to(DEVICE), gta_lbl.to(DEVICE), city_img.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 1. Generate Pseudo-Labels for Cityscapes\n",
        "        with torch.no_grad():\n",
        "            city_out = model(city_img)\n",
        "            if isinstance(city_out, tuple): city_out = city_out[0]\n",
        "\n",
        "            probs = torch.softmax(city_out, dim=1)\n",
        "            max_probs, city_pseudo_lbl = torch.max(probs, dim=1)\n",
        "\n",
        "            # --- CRITICAL FIX: THRESHOLDING ---\n",
        "\n",
        "            city_pseudo_lbl[max_probs < CONFIDENCE_THRESHOLD] = 255\n",
        "\n",
        "        # 2. Mix Images (ClassMix)\n",
        "        mixed_img, mixed_lbl = dacs_mix(gta_img, gta_lbl, city_img, city_pseudo_lbl)\n",
        "\n",
        "        # 3. Train on Mixed Images\n",
        "        out = model(mixed_img)\n",
        "\n",
        "        # Loss calculation (Standard BiSeNet Multi-head Loss)\n",
        "        loss = criterion(out[0], mixed_lbl)\n",
        "        loss += 0.1 * criterion(out[1], mixed_lbl) + 0.1 * criterion(out[2], mixed_lbl)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Step [{i}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    if hasattr(optimizer, 'get_last_lr'): scheduler.step()\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, SAVE_PATH)\n",
        "    print(f\"Epoch {epoch+1} Saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpidnLFh5jeT",
        "outputId": "0518c2cc-3049-4391-dd2d-fac82bad2a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Advanced DACS Training (With Thresholding)...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 191MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:01<00:00, 175MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Step [0/313] Loss: 4.7641\n",
            "Epoch [1/50] Step [50/313] Loss: 0.7572\n",
            "Epoch [1/50] Step [100/313] Loss: 0.5669\n",
            "Epoch [1/50] Step [150/313] Loss: 0.4181\n",
            "Epoch [1/50] Step [200/313] Loss: 0.4624\n",
            "Epoch [1/50] Step [250/313] Loss: 0.4083\n",
            "Epoch [1/50] Step [300/313] Loss: 0.3949\n",
            "Epoch 1 Saved.\n",
            "Epoch [2/50] Step [0/313] Loss: 0.5912\n",
            "Epoch [2/50] Step [50/313] Loss: 0.3660\n",
            "Epoch [2/50] Step [100/313] Loss: 0.3352\n",
            "Epoch [2/50] Step [150/313] Loss: 0.3606\n",
            "Epoch [2/50] Step [200/313] Loss: 0.3330\n",
            "Epoch [2/50] Step [250/313] Loss: 0.3450\n",
            "Epoch [2/50] Step [300/313] Loss: 0.3020\n",
            "Epoch 2 Saved.\n",
            "Epoch [3/50] Step [0/313] Loss: 0.3918\n",
            "Epoch [3/50] Step [50/313] Loss: 0.3339\n",
            "Epoch [3/50] Step [100/313] Loss: 0.3510\n",
            "Epoch [3/50] Step [150/313] Loss: 0.3672\n",
            "Epoch [3/50] Step [200/313] Loss: 0.4017\n",
            "Epoch [3/50] Step [250/313] Loss: 0.2865\n",
            "Epoch [3/50] Step [300/313] Loss: 0.2354\n",
            "Epoch 3 Saved.\n",
            "Epoch [4/50] Step [0/313] Loss: 0.2596\n",
            "Epoch [4/50] Step [50/313] Loss: 0.3310\n",
            "Epoch [4/50] Step [100/313] Loss: 0.2810\n",
            "Epoch [4/50] Step [150/313] Loss: 0.2786\n",
            "Epoch [4/50] Step [200/313] Loss: 0.2509\n",
            "Epoch [4/50] Step [250/313] Loss: 0.2959\n",
            "Epoch [4/50] Step [300/313] Loss: 0.2114\n",
            "Epoch 4 Saved.\n",
            "Epoch [5/50] Step [0/313] Loss: 0.3263\n",
            "Epoch [5/50] Step [50/313] Loss: 0.2363\n",
            "Epoch [5/50] Step [100/313] Loss: 0.2075\n",
            "Epoch [5/50] Step [150/313] Loss: 0.2456\n",
            "Epoch [5/50] Step [200/313] Loss: 0.2873\n",
            "Epoch [5/50] Step [250/313] Loss: 0.2930\n",
            "Epoch [5/50] Step [300/313] Loss: 0.2487\n",
            "Epoch 5 Saved.\n",
            "Epoch [6/50] Step [0/313] Loss: 0.1987\n",
            "Epoch [6/50] Step [50/313] Loss: 0.2824\n",
            "Epoch [6/50] Step [100/313] Loss: 0.2181\n",
            "Epoch [6/50] Step [150/313] Loss: 0.3671\n",
            "Epoch [6/50] Step [200/313] Loss: 0.2934\n",
            "Epoch [6/50] Step [250/313] Loss: 0.3386\n",
            "Epoch [6/50] Step [300/313] Loss: 0.2918\n",
            "Epoch 6 Saved.\n",
            "Epoch [7/50] Step [0/313] Loss: 0.2286\n",
            "Epoch [7/50] Step [50/313] Loss: 0.1979\n",
            "Epoch [7/50] Step [100/313] Loss: 0.2376\n",
            "Epoch [7/50] Step [150/313] Loss: 0.2217\n",
            "Epoch [7/50] Step [200/313] Loss: 0.2337\n",
            "Epoch [7/50] Step [250/313] Loss: 0.2605\n",
            "Epoch [7/50] Step [300/313] Loss: 0.3059\n",
            "Epoch 7 Saved.\n",
            "Epoch [8/50] Step [0/313] Loss: 0.2943\n",
            "Epoch [8/50] Step [50/313] Loss: 0.2793\n",
            "Epoch [8/50] Step [100/313] Loss: 0.1551\n",
            "Epoch [8/50] Step [150/313] Loss: 0.2099\n",
            "Epoch [8/50] Step [200/313] Loss: 0.4788\n",
            "Epoch [8/50] Step [250/313] Loss: 0.1948\n",
            "Epoch [8/50] Step [300/313] Loss: 0.2155\n",
            "Epoch 8 Saved.\n",
            "Epoch [9/50] Step [0/313] Loss: 0.2584\n",
            "Epoch [9/50] Step [50/313] Loss: 0.1995\n",
            "Epoch [9/50] Step [100/313] Loss: 0.1765\n",
            "Epoch [9/50] Step [150/313] Loss: 0.2196\n",
            "Epoch [9/50] Step [200/313] Loss: 0.2476\n",
            "Epoch [9/50] Step [250/313] Loss: 0.1857\n",
            "Epoch [9/50] Step [300/313] Loss: 0.2873\n",
            "Epoch 9 Saved.\n",
            "Epoch [10/50] Step [0/313] Loss: 0.1661\n",
            "Epoch [10/50] Step [50/313] Loss: 0.2435\n",
            "Epoch [10/50] Step [100/313] Loss: 0.2169\n",
            "Epoch [10/50] Step [150/313] Loss: 0.2249\n",
            "Epoch [10/50] Step [200/313] Loss: 0.1971\n",
            "Epoch [10/50] Step [250/313] Loss: 0.1786\n",
            "Epoch [10/50] Step [300/313] Loss: 0.2647\n",
            "Epoch 10 Saved.\n",
            "Epoch [11/50] Step [0/313] Loss: 0.2680\n",
            "Epoch [11/50] Step [50/313] Loss: 0.2367\n",
            "Epoch [11/50] Step [100/313] Loss: 0.1655\n",
            "Epoch [11/50] Step [150/313] Loss: 0.2205\n",
            "Epoch [11/50] Step [200/313] Loss: 0.3253\n",
            "Epoch [11/50] Step [250/313] Loss: 0.1865\n",
            "Epoch [11/50] Step [300/313] Loss: 0.2405\n",
            "Epoch 11 Saved.\n",
            "Epoch [12/50] Step [0/313] Loss: 0.2000\n",
            "Epoch [12/50] Step [50/313] Loss: 0.2505\n",
            "Epoch [12/50] Step [100/313] Loss: 0.2357\n",
            "Epoch [12/50] Step [150/313] Loss: 0.1618\n",
            "Epoch [12/50] Step [200/313] Loss: 0.2412\n",
            "Epoch [12/50] Step [250/313] Loss: 0.2008\n",
            "Epoch [12/50] Step [300/313] Loss: 0.2025\n",
            "Epoch 12 Saved.\n",
            "Epoch [13/50] Step [0/313] Loss: 0.2369\n",
            "Epoch [13/50] Step [50/313] Loss: 0.2234\n",
            "Epoch [13/50] Step [100/313] Loss: 0.1798\n",
            "Epoch [13/50] Step [150/313] Loss: 0.1833\n",
            "Epoch [13/50] Step [200/313] Loss: 0.2320\n",
            "Epoch [13/50] Step [250/313] Loss: 0.1680\n",
            "Epoch [13/50] Step [300/313] Loss: 0.1820\n",
            "Epoch 13 Saved.\n",
            "Epoch [14/50] Step [0/313] Loss: 0.1571\n",
            "Epoch [14/50] Step [50/313] Loss: 0.1951\n",
            "Epoch [14/50] Step [100/313] Loss: 0.1840\n",
            "Epoch [14/50] Step [150/313] Loss: 0.2508\n",
            "Epoch [14/50] Step [200/313] Loss: 0.1479\n",
            "Epoch [14/50] Step [250/313] Loss: 0.1515\n",
            "Epoch [14/50] Step [300/313] Loss: 0.1757\n",
            "Epoch 14 Saved.\n",
            "Epoch [15/50] Step [0/313] Loss: 0.1812\n",
            "Epoch [15/50] Step [50/313] Loss: 0.2182\n",
            "Epoch [15/50] Step [100/313] Loss: 0.2060\n",
            "Epoch [15/50] Step [150/313] Loss: 0.1544\n",
            "Epoch [15/50] Step [200/313] Loss: 0.1547\n",
            "Epoch [15/50] Step [250/313] Loss: 0.2400\n",
            "Epoch [15/50] Step [300/313] Loss: 0.2449\n",
            "Epoch 15 Saved.\n",
            "Epoch [16/50] Step [0/313] Loss: 0.1758\n",
            "Epoch [16/50] Step [50/313] Loss: 0.1993\n",
            "Epoch [16/50] Step [100/313] Loss: 0.2117\n",
            "Epoch [16/50] Step [150/313] Loss: 0.2223\n",
            "Epoch [16/50] Step [200/313] Loss: 0.1878\n",
            "Epoch [16/50] Step [250/313] Loss: 0.1649\n",
            "Epoch [16/50] Step [300/313] Loss: 0.5116\n",
            "Epoch 16 Saved.\n",
            "Epoch [17/50] Step [0/313] Loss: 0.2525\n",
            "Epoch [17/50] Step [50/313] Loss: 0.2498\n",
            "Epoch [17/50] Step [100/313] Loss: 0.1995\n",
            "Epoch [17/50] Step [150/313] Loss: 0.1817\n",
            "Epoch [17/50] Step [200/313] Loss: 0.1698\n",
            "Epoch [17/50] Step [250/313] Loss: 0.2415\n",
            "Epoch [17/50] Step [300/313] Loss: 0.1446\n",
            "Epoch 17 Saved.\n",
            "Epoch [18/50] Step [0/313] Loss: 0.2279\n",
            "Epoch [18/50] Step [50/313] Loss: 0.2341\n",
            "Epoch [18/50] Step [100/313] Loss: 0.1722\n",
            "Epoch [18/50] Step [150/313] Loss: 0.2119\n",
            "Epoch [18/50] Step [200/313] Loss: 0.1608\n",
            "Epoch [18/50] Step [250/313] Loss: 0.1497\n",
            "Epoch [18/50] Step [300/313] Loss: 0.2001\n",
            "Epoch 18 Saved.\n",
            "Epoch [19/50] Step [0/313] Loss: 0.2618\n",
            "Epoch [19/50] Step [50/313] Loss: 0.1504\n",
            "Epoch [19/50] Step [100/313] Loss: 0.1634\n",
            "Epoch [19/50] Step [150/313] Loss: 0.1979\n",
            "Epoch [19/50] Step [200/313] Loss: 0.2137\n",
            "Epoch [19/50] Step [250/313] Loss: 0.2466\n",
            "Epoch [19/50] Step [300/313] Loss: 0.1632\n",
            "Epoch 19 Saved.\n",
            "Epoch [20/50] Step [0/313] Loss: 0.1724\n",
            "Epoch [20/50] Step [50/313] Loss: 0.1813\n",
            "Epoch [20/50] Step [100/313] Loss: 0.1866\n",
            "Epoch [20/50] Step [150/313] Loss: 0.2734\n",
            "Epoch [20/50] Step [200/313] Loss: 0.1904\n",
            "Epoch [20/50] Step [250/313] Loss: 0.1384\n",
            "Epoch [20/50] Step [300/313] Loss: 0.1566\n",
            "Epoch 20 Saved.\n",
            "Epoch [21/50] Step [0/313] Loss: 0.3352\n",
            "Epoch [21/50] Step [50/313] Loss: 0.2617\n",
            "Epoch [21/50] Step [100/313] Loss: 0.2430\n",
            "Epoch [21/50] Step [150/313] Loss: 0.2053\n",
            "Epoch [21/50] Step [200/313] Loss: 0.1888\n",
            "Epoch [21/50] Step [250/313] Loss: 0.1943\n",
            "Epoch [21/50] Step [300/313] Loss: 0.2339\n",
            "Epoch 21 Saved.\n",
            "Epoch [22/50] Step [0/313] Loss: 0.1923\n",
            "Epoch [22/50] Step [50/313] Loss: 0.1598\n",
            "Epoch [22/50] Step [100/313] Loss: 0.2746\n",
            "Epoch [22/50] Step [150/313] Loss: 0.1468\n",
            "Epoch [22/50] Step [200/313] Loss: 0.2442\n",
            "Epoch [22/50] Step [250/313] Loss: 0.1392\n",
            "Epoch [22/50] Step [300/313] Loss: 0.1745\n",
            "Epoch 22 Saved.\n",
            "Epoch [23/50] Step [0/313] Loss: 0.1404\n",
            "Epoch [23/50] Step [50/313] Loss: 0.1997\n",
            "Epoch [23/50] Step [100/313] Loss: 0.1770\n",
            "Epoch [23/50] Step [150/313] Loss: 0.2150\n",
            "Epoch [23/50] Step [200/313] Loss: 0.2324\n",
            "Epoch [23/50] Step [250/313] Loss: 0.1731\n",
            "Epoch [23/50] Step [300/313] Loss: 0.2050\n",
            "Epoch 23 Saved.\n",
            "Epoch [24/50] Step [0/313] Loss: 0.1862\n",
            "Epoch [24/50] Step [50/313] Loss: 0.2859\n",
            "Epoch [24/50] Step [100/313] Loss: 0.2515\n",
            "Epoch [24/50] Step [150/313] Loss: 0.2070\n",
            "Epoch [24/50] Step [200/313] Loss: 0.1672\n",
            "Epoch [24/50] Step [250/313] Loss: 0.2318\n",
            "Epoch [24/50] Step [300/313] Loss: 0.1841\n",
            "Epoch 24 Saved.\n",
            "Epoch [25/50] Step [0/313] Loss: 0.1632\n",
            "Epoch [25/50] Step [50/313] Loss: 0.2017\n",
            "Epoch [25/50] Step [100/313] Loss: 0.1776\n",
            "Epoch [25/50] Step [150/313] Loss: 0.1570\n",
            "Epoch [25/50] Step [200/313] Loss: 0.2439\n",
            "Epoch [25/50] Step [250/313] Loss: 0.1881\n",
            "Epoch [25/50] Step [300/313] Loss: 0.2618\n",
            "Epoch 25 Saved.\n",
            "Epoch [26/50] Step [0/313] Loss: 0.1679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# Configuration\n",
        "CHECKPOINT_NAME = 'bisenet_dacs_checkpoint.pth'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 8\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "GTA_PATH = '/content/dataset/project_data/gta5'\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "SAVE_PATH = f'/content/drive/MyDrive/semseg/{CHECKPOINT_NAME}'\n",
        "\n",
        "class GTA5_City_Dataset(Dataset):\n",
        "    def __init__(self, gta_root, city_root):\n",
        "        self.gta_images_dir = os.path.join(gta_root, 'images')\n",
        "        self.gta_masks_dir = os.path.join(gta_root, 'labels')\n",
        "        self.gta_images = sorted(os.listdir(self.gta_images_dir))\n",
        "\n",
        "        self.city_images_dir = os.path.join(city_root, 'leftImg8bit', 'train')\n",
        "        self.city_images = []\n",
        "        if os.path.exists(self.city_images_dir):\n",
        "            for city in os.listdir(self.city_images_dir):\n",
        "                c_path = os.path.join(self.city_images_dir, city)\n",
        "                if os.path.isdir(c_path):\n",
        "                    for f in os.listdir(c_path):\n",
        "                        if f.endswith('_leftImg8bit.png'):\n",
        "                            self.city_images.append(os.path.join(c_path, f))\n",
        "\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.id_mapping = {\n",
        "            7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "            19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "            26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18\n",
        "        }\n",
        "\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self): return len(self.gta_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gta_path = os.path.join(self.gta_images_dir, self.gta_images[idx])\n",
        "        mask_path = os.path.join(self.gta_masks_dir, self.gta_images[idx])\n",
        "\n",
        "        gta_img = Image.open(gta_path).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "        gta_mask = Image.open(mask_path).resize((1280, 720), Image.NEAREST)\n",
        "\n",
        "        rand_idx = random.randint(0, len(self.city_images) - 1)\n",
        "        city_img = Image.open(self.city_images[rand_idx]).convert('RGB').resize((1280, 720), Image.BILINEAR)\n",
        "\n",
        "        gta_t = self.normalize(self.to_tensor(gta_img))\n",
        "        city_t = self.normalize(self.to_tensor(city_img))\n",
        "\n",
        "        mask_np = np.array(gta_mask)\n",
        "        gta_lbl = np.full(mask_np.shape, 255, dtype=np.uint8)\n",
        "        for k, v in self.id_mapping.items(): gta_lbl[mask_np == k] = v\n",
        "\n",
        "        return gta_t, torch.from_numpy(gta_lbl).long(), city_t\n",
        "\n",
        "def dacs_mix(gta_img, gta_lbl, city_img, city_pseudo_lbl):\n",
        "    batch_size = gta_img.shape[0]\n",
        "    mixed_img = city_img.clone()\n",
        "    mixed_lbl = city_pseudo_lbl.clone()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        classes_in_img = torch.unique(gta_lbl[i])\n",
        "        classes_in_img = classes_in_img[classes_in_img != 255]\n",
        "\n",
        "        if len(classes_in_img) > 0:\n",
        "            n_classes = len(classes_in_img)\n",
        "            perm = torch.randperm(n_classes)\n",
        "            selected_classes = classes_in_img[perm[:(n_classes + 1) // 2]]\n",
        "\n",
        "            mask = torch.zeros_like(gta_lbl[i]).bool()\n",
        "            for c in selected_classes:\n",
        "                mask = mask | (gta_lbl[i] == c)\n",
        "\n",
        "            mixed_img[i, :, mask] = gta_img[i, :, mask]\n",
        "            mixed_lbl[i, mask] = gta_lbl[i, mask]\n",
        "\n",
        "    return mixed_img, mixed_lbl\n",
        "\n",
        "print(\"Resuming DACS Training...\")\n",
        "\n",
        "model = BiSeNet(num_classes=19, context_path='resnet18').to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "dataset = GTA5_City_Dataset(GTA_PATH, CITYSCAPES_PATH)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(\"Found checkpoint. Loading...\")\n",
        "    checkpoint = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "    print(f\"Starting from Epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting from Epoch 0\")\n",
        "    start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    for i, (gta_img, gta_lbl, city_img) in enumerate(loader):\n",
        "        gta_img = gta_img.to(DEVICE)\n",
        "        gta_lbl = gta_lbl.to(DEVICE)\n",
        "        city_img = city_img.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            city_out = model(city_img)\n",
        "            if isinstance(city_out, tuple): city_out = city_out[0]\n",
        "            city_pseudo_lbl = torch.argmax(city_out, dim=1)\n",
        "\n",
        "        mixed_img, mixed_lbl = dacs_mix(gta_img, gta_lbl, city_img, city_pseudo_lbl)\n",
        "\n",
        "        out = model(mixed_img)\n",
        "        loss = criterion(out[0], mixed_lbl) + 0.1 * criterion(out[1], mixed_lbl) + 0.1 * criterion(out[2], mixed_lbl)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Step [{i}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, SAVE_PATH)\n",
        "    print(f\"Epoch {epoch+1} Saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2Du1r1wsI1",
        "outputId": "fb9c4d5f-eaad-4089-bbdd-494b84a6760c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming DACS Training...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 164MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:00<00:00, 182MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found checkpoint. Loading...\n",
            "Starting from Epoch 28\n",
            "Epoch [29/50] Step [0/313] Loss: 0.3165\n",
            "Epoch [29/50] Step [50/313] Loss: 0.2518\n",
            "Epoch [29/50] Step [100/313] Loss: 0.2992\n",
            "Epoch [29/50] Step [150/313] Loss: 0.2459\n",
            "Epoch [29/50] Step [200/313] Loss: 0.4181\n",
            "Epoch [29/50] Step [250/313] Loss: 0.2636\n",
            "Epoch [29/50] Step [300/313] Loss: 0.3092\n",
            "Epoch 29 Saved.\n",
            "Epoch [30/50] Step [0/313] Loss: 0.2924\n",
            "Epoch [30/50] Step [50/313] Loss: 0.2786\n",
            "Epoch [30/50] Step [100/313] Loss: 0.2836\n",
            "Epoch [30/50] Step [150/313] Loss: 0.2963\n",
            "Epoch [30/50] Step [200/313] Loss: 0.3433\n",
            "Epoch [30/50] Step [250/313] Loss: 0.3134\n",
            "Epoch [30/50] Step [300/313] Loss: 0.3390\n",
            "Epoch 30 Saved.\n",
            "Epoch [31/50] Step [0/313] Loss: 0.2895\n",
            "Epoch [31/50] Step [50/313] Loss: 0.2003\n",
            "Epoch [31/50] Step [100/313] Loss: 0.3458\n",
            "Epoch [31/50] Step [150/313] Loss: 0.2423\n",
            "Epoch [31/50] Step [200/313] Loss: 0.2484\n",
            "Epoch [31/50] Step [250/313] Loss: 0.4212\n",
            "Epoch [31/50] Step [300/313] Loss: 0.3924\n",
            "Epoch 31 Saved.\n",
            "Epoch [32/50] Step [0/313] Loss: 0.2846\n",
            "Epoch [32/50] Step [50/313] Loss: 0.3081\n",
            "Epoch [32/50] Step [100/313] Loss: 0.2481\n",
            "Epoch [32/50] Step [150/313] Loss: 0.2796\n",
            "Epoch [32/50] Step [200/313] Loss: 0.2738\n",
            "Epoch [32/50] Step [250/313] Loss: 0.3126\n",
            "Epoch [32/50] Step [300/313] Loss: 0.3026\n",
            "Epoch 32 Saved.\n",
            "Epoch [33/50] Step [0/313] Loss: 0.2557\n",
            "Epoch [33/50] Step [50/313] Loss: 0.2935\n",
            "Epoch [33/50] Step [100/313] Loss: 0.2880\n",
            "Epoch [33/50] Step [150/313] Loss: 0.3778\n",
            "Epoch [33/50] Step [200/313] Loss: 0.3476\n",
            "Epoch [33/50] Step [250/313] Loss: 0.3701\n",
            "Epoch [33/50] Step [300/313] Loss: 0.2772\n",
            "Epoch 33 Saved.\n",
            "Epoch [34/50] Step [0/313] Loss: 0.2761\n",
            "Epoch [34/50] Step [50/313] Loss: 0.3305\n",
            "Epoch [34/50] Step [100/313] Loss: 0.2772\n",
            "Epoch [34/50] Step [150/313] Loss: 0.2149\n",
            "Epoch [34/50] Step [200/313] Loss: 0.3090\n",
            "Epoch [34/50] Step [250/313] Loss: 0.3166\n",
            "Epoch [34/50] Step [300/313] Loss: 0.2604\n",
            "Epoch 34 Saved.\n",
            "Epoch [35/50] Step [0/313] Loss: 0.2598\n",
            "Epoch [35/50] Step [50/313] Loss: 0.3097\n",
            "Epoch [35/50] Step [100/313] Loss: 0.2839\n",
            "Epoch [35/50] Step [150/313] Loss: 0.2695\n",
            "Epoch [35/50] Step [200/313] Loss: 0.2749\n",
            "Epoch [35/50] Step [250/313] Loss: 0.3009\n",
            "Epoch [35/50] Step [300/313] Loss: 0.3098\n",
            "Epoch 35 Saved.\n",
            "Epoch [36/50] Step [0/313] Loss: 0.3218\n",
            "Epoch [36/50] Step [50/313] Loss: 0.2776\n",
            "Epoch [36/50] Step [100/313] Loss: 0.2929\n",
            "Epoch [36/50] Step [150/313] Loss: 0.3023\n",
            "Epoch [36/50] Step [200/313] Loss: 0.2785\n",
            "Epoch [36/50] Step [250/313] Loss: 0.2949\n",
            "Epoch [36/50] Step [300/313] Loss: 0.3018\n",
            "Epoch 36 Saved.\n",
            "Epoch [37/50] Step [0/313] Loss: 0.2986\n",
            "Epoch [37/50] Step [50/313] Loss: 0.2662\n",
            "Epoch [37/50] Step [100/313] Loss: 0.3105\n",
            "Epoch [37/50] Step [150/313] Loss: 0.2879\n",
            "Epoch [37/50] Step [200/313] Loss: 0.3040\n",
            "Epoch [37/50] Step [250/313] Loss: 0.3172\n",
            "Epoch [37/50] Step [300/313] Loss: 0.2974\n",
            "Epoch 37 Saved.\n",
            "Epoch [38/50] Step [0/313] Loss: 0.3980\n",
            "Epoch [38/50] Step [50/313] Loss: 0.2548\n",
            "Epoch [38/50] Step [100/313] Loss: 0.2489\n",
            "Epoch [38/50] Step [150/313] Loss: 0.3243\n",
            "Epoch [38/50] Step [200/313] Loss: 0.2672\n",
            "Epoch [38/50] Step [250/313] Loss: 0.2533\n",
            "Epoch [38/50] Step [300/313] Loss: 0.3162\n",
            "Epoch 38 Saved.\n",
            "Epoch [39/50] Step [0/313] Loss: 0.2546\n",
            "Epoch [39/50] Step [50/313] Loss: 0.3174\n",
            "Epoch [39/50] Step [100/313] Loss: 0.2673\n",
            "Epoch [39/50] Step [150/313] Loss: 0.2805\n",
            "Epoch [39/50] Step [200/313] Loss: 0.2927\n",
            "Epoch [39/50] Step [250/313] Loss: 0.2609\n",
            "Epoch [39/50] Step [300/313] Loss: 0.2870\n",
            "Epoch 39 Saved.\n",
            "Epoch [40/50] Step [0/313] Loss: 0.3575\n",
            "Epoch [40/50] Step [50/313] Loss: 0.2478\n",
            "Epoch [40/50] Step [100/313] Loss: 0.2449\n",
            "Epoch [40/50] Step [150/313] Loss: 0.2635\n",
            "Epoch [40/50] Step [200/313] Loss: 0.2860\n",
            "Epoch [40/50] Step [250/313] Loss: 0.3121\n",
            "Epoch [40/50] Step [300/313] Loss: 0.2672\n",
            "Epoch 40 Saved.\n",
            "Epoch [41/50] Step [0/313] Loss: 0.2616\n",
            "Epoch [41/50] Step [50/313] Loss: 0.2895\n",
            "Epoch [41/50] Step [100/313] Loss: 0.2788\n",
            "Epoch [41/50] Step [150/313] Loss: 0.3028\n",
            "Epoch [41/50] Step [200/313] Loss: 0.3213\n",
            "Epoch [41/50] Step [250/313] Loss: 0.2868\n",
            "Epoch [41/50] Step [300/313] Loss: 0.2681\n",
            "Epoch 41 Saved.\n",
            "Epoch [42/50] Step [0/313] Loss: 0.2809\n",
            "Epoch [42/50] Step [50/313] Loss: 0.2308\n",
            "Epoch [42/50] Step [100/313] Loss: 0.2976\n",
            "Epoch [42/50] Step [150/313] Loss: 0.3299\n",
            "Epoch [42/50] Step [200/313] Loss: 0.2801\n",
            "Epoch [42/50] Step [250/313] Loss: 0.2728\n",
            "Epoch [42/50] Step [300/313] Loss: 0.3149\n",
            "Epoch 42 Saved.\n",
            "Epoch [43/50] Step [0/313] Loss: 0.2866\n",
            "Epoch [43/50] Step [50/313] Loss: 0.2574\n",
            "Epoch [43/50] Step [100/313] Loss: 0.2311\n",
            "Epoch [43/50] Step [150/313] Loss: 0.2527\n",
            "Epoch [43/50] Step [200/313] Loss: 0.2849\n",
            "Epoch [43/50] Step [250/313] Loss: 0.2821\n",
            "Epoch [43/50] Step [300/313] Loss: 0.2681\n",
            "Epoch 43 Saved.\n",
            "Epoch [44/50] Step [0/313] Loss: 0.3102\n",
            "Epoch [44/50] Step [50/313] Loss: 0.2574\n",
            "Epoch [44/50] Step [100/313] Loss: 0.2803\n",
            "Epoch [44/50] Step [150/313] Loss: 0.2738\n",
            "Epoch [44/50] Step [200/313] Loss: 0.4258\n",
            "Epoch [44/50] Step [250/313] Loss: 0.2414\n",
            "Epoch [44/50] Step [300/313] Loss: 0.2618\n",
            "Epoch 44 Saved.\n",
            "Epoch [45/50] Step [0/313] Loss: 0.2152\n",
            "Epoch [45/50] Step [50/313] Loss: 0.3149\n",
            "Epoch [45/50] Step [100/313] Loss: 0.2343\n",
            "Epoch [45/50] Step [150/313] Loss: 0.2961\n",
            "Epoch [45/50] Step [200/313] Loss: 0.2846\n",
            "Epoch [45/50] Step [250/313] Loss: 0.2605\n",
            "Epoch [45/50] Step [300/313] Loss: 0.2724\n",
            "Epoch 45 Saved.\n",
            "Epoch [46/50] Step [0/313] Loss: 0.2544\n",
            "Epoch [46/50] Step [50/313] Loss: 0.2854\n",
            "Epoch [46/50] Step [100/313] Loss: 0.2692\n",
            "Epoch [46/50] Step [150/313] Loss: 0.2448\n",
            "Epoch [46/50] Step [200/313] Loss: 0.2504\n",
            "Epoch [46/50] Step [250/313] Loss: 0.2562\n",
            "Epoch [46/50] Step [300/313] Loss: 0.2159\n",
            "Epoch 46 Saved.\n",
            "Epoch [47/50] Step [0/313] Loss: 0.2562\n",
            "Epoch [47/50] Step [50/313] Loss: 0.3720\n",
            "Epoch [47/50] Step [100/313] Loss: 0.2883\n",
            "Epoch [47/50] Step [150/313] Loss: 0.3084\n",
            "Epoch [47/50] Step [200/313] Loss: 0.2717\n",
            "Epoch [47/50] Step [250/313] Loss: 0.3230\n",
            "Epoch [47/50] Step [300/313] Loss: 0.2809\n",
            "Epoch 47 Saved.\n",
            "Epoch [48/50] Step [0/313] Loss: 0.3141\n",
            "Epoch [48/50] Step [50/313] Loss: 0.2465\n",
            "Epoch [48/50] Step [100/313] Loss: 0.2695\n",
            "Epoch [48/50] Step [150/313] Loss: 0.2965\n",
            "Epoch [48/50] Step [200/313] Loss: 0.2888\n",
            "Epoch [48/50] Step [250/313] Loss: 0.2887\n",
            "Epoch [48/50] Step [300/313] Loss: 0.2964\n",
            "Epoch 48 Saved.\n",
            "Epoch [49/50] Step [0/313] Loss: 0.3069\n",
            "Epoch [49/50] Step [50/313] Loss: 0.3188\n",
            "Epoch [49/50] Step [100/313] Loss: 0.2772\n",
            "Epoch [49/50] Step [150/313] Loss: 0.2528\n",
            "Epoch [49/50] Step [200/313] Loss: 0.2472\n",
            "Epoch [49/50] Step [250/313] Loss: 0.2713\n",
            "Epoch [49/50] Step [300/313] Loss: 0.3119\n",
            "Epoch 49 Saved.\n",
            "Epoch [50/50] Step [0/313] Loss: 0.3012\n",
            "Epoch [50/50] Step [50/313] Loss: 0.3267\n",
            "Epoch [50/50] Step [100/313] Loss: 0.3124\n",
            "Epoch [50/50] Step [150/313] Loss: 0.3302\n",
            "Epoch [50/50] Step [200/313] Loss: 0.3057\n",
            "Epoch [50/50] Step [250/313] Loss: 0.2717\n",
            "Epoch [50/50] Step [300/313] Loss: 0.2270\n",
            "Epoch 50 Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# --- PATH SETUP ---\n",
        "if os.path.exists(\"/content/MLDL2024_project1\"):\n",
        "    sys.path.append(\"/content/MLDL2024_project1\")\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# --- CONFIG ---\n",
        "# POINTING TO THE CORRECT FILE (The one you trained for 2 days)\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/semseg/bisenet_dacs_thresholded.pth\"\n",
        "CITY_PATH = \"/content/dataset/project_data/cityscapes\"\n",
        "NUM_CLASSES = 19\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- DATASET ---\n",
        "class CityscapesValDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.img_dir = os.path.join(root, \"leftImg8bit\", \"val\")\n",
        "        self.lbl_dir = os.path.join(root, \"gtFine\", \"val\")\n",
        "        self.imgs = []\n",
        "        self.lbls = []\n",
        "\n",
        "        if os.path.exists(self.img_dir):\n",
        "            for city in sorted(os.listdir(self.img_dir)):\n",
        "                img_path = os.path.join(self.img_dir, city)\n",
        "                lbl_path = os.path.join(self.lbl_dir, city)\n",
        "                for f in sorted(os.listdir(img_path)):\n",
        "                    if f.endswith(\"_leftImg8bit.png\"):\n",
        "                        self.imgs.append(os.path.join(img_path, f))\n",
        "                        self.lbls.append(os.path.join(lbl_path, f.replace(\"_leftImg8bit.png\", \"_gtFine_labelTrainIds.png\")))\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((512, 1024)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.imgs[idx]).convert(\"RGB\")\n",
        "        lbl = Image.open(self.lbls[idx]).resize((1024, 512), Image.NEAREST)\n",
        "        img = self.transform(img)\n",
        "        lbl = torch.from_numpy(np.array(lbl)).long()\n",
        "        return img, lbl\n",
        "\n",
        "# --- EVALUATION ---\n",
        "print(f\"Loading Checkpoint: {CHECKPOINT_PATH}\")\n",
        "model = BiSeNet(num_classes=NUM_CLASSES, context_path=\"resnet18\").to(DEVICE)\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
        "    # The 'thresholded' script saved simple 'model_state_dict'\n",
        "    if 'model_state_dict' in ckpt:\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        print(\"✅ SUCCESS: Model weights loaded.\")\n",
        "    else:\n",
        "        model.load_state_dict(ckpt)\n",
        "        print(\"Model weights loaded (Direct).\")\n",
        "else:\n",
        "    print(f\"Error: {CHECKPOINT_PATH} not found.\")\n",
        "    sys.exit()\n",
        "\n",
        "model.eval()\n",
        "loader = DataLoader(CityscapesValDataset(CITY_PATH), batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "hist = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
        "print(\"Starting Evaluation...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, lbl in tqdm(loader):\n",
        "        img = img.to(DEVICE)\n",
        "\n",
        "        # Robust inference\n",
        "        output = model(img)\n",
        "        if isinstance(output, (list, tuple)):\n",
        "            output = output[0]\n",
        "\n",
        "        preds = torch.argmax(output, dim=1).cpu().numpy()\n",
        "        lbl = lbl.numpy()\n",
        "\n",
        "        if preds.ndim == 3: preds = preds[0]\n",
        "        if lbl.ndim == 3: lbl = lbl[0]\n",
        "\n",
        "        mask = (lbl >= 0) & (lbl < NUM_CLASSES)\n",
        "        hist += np.bincount(\n",
        "            NUM_CLASSES * lbl[mask].astype(int) + preds[mask],\n",
        "            minlength=NUM_CLASSES ** 2\n",
        "        ).reshape(NUM_CLASSES, NUM_CLASSES)\n",
        "\n",
        "iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "miou = np.nanmean(iou) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(f\"Final DACS (Thresholded) mIoU: {miou:.2f}%\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "CLASSES = [\n",
        "    \"Road\", \"Sidewalk\", \"Building\", \"Wall\", \"Fence\", \"Pole\",\n",
        "    \"Traffic Light\", \"Traffic Sign\", \"Vegetation\", \"Terrain\", \"Sky\",\n",
        "    \"Person\", \"Rider\", \"Car\", \"Truck\", \"Bus\", \"Train\", \"Motorcycle\", \"Bicycle\"\n",
        "]\n",
        "print(\"-\" * 30)\n",
        "for i, name in enumerate(CLASSES):\n",
        "    print(f\"{name:15s}: {iou[i]*100:.2f}%\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPaqY8z650mM",
        "outputId": "1062f6f1-70eb-4798-a589-445d15fa5453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Checkpoint: /content/drive/MyDrive/semseg/bisenet_dacs_thresholded.pth\n",
            "✅ SUCCESS: Model weights loaded.\n",
            "Starting Evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:56<00:00,  8.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Final DACS (Thresholded) mIoU: 24.83%\n",
            "==============================\n",
            "------------------------------\n",
            "Road           : 86.92%\n",
            "Sidewalk       : 16.52%\n",
            "Building       : 57.81%\n",
            "Wall           : 7.16%\n",
            "Fence          : 11.77%\n",
            "Pole           : 16.09%\n",
            "Traffic Light  : 8.06%\n",
            "Traffic Sign   : 6.84%\n",
            "Vegetation     : 66.26%\n",
            "Terrain        : 12.73%\n",
            "Sky            : 50.30%\n",
            "Person         : 33.31%\n",
            "Rider          : 3.52%\n",
            "Car            : 72.31%\n",
            "Truck          : 11.48%\n",
            "Bus            : 4.44%\n",
            "Train          : 0.00%\n",
            "Motorcycle     : 6.23%\n",
            "Bicycle        : 0.00%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}