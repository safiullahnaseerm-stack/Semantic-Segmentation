{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrLeKm75weIw",
        "outputId": "f3252b59-aa70-4a74-f147-b4e2710dab83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Restoring Cityscapes...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Restore Dataset\n",
        "if not os.path.exists('/content/dataset/project_data/cityscapes'):\n",
        "    print(\"Restoring Cityscapes...\")\n",
        "    zip_path_1 = '/content/drive/MyDrive/semseg/project_data.zip'\n",
        "    zip_path_2 = '/content/drive/MyDrive/project_data.zip'\n",
        "\n",
        "    if os.path.exists(zip_path_1):\n",
        "        shutil.unpack_archive(zip_path_1, '/content/dataset')\n",
        "    elif os.path.exists(zip_path_2):\n",
        "        shutil.unpack_archive(zip_path_2, '/content/dataset')\n",
        "    else:\n",
        "        print(\"Dataset zip not found. Please check paths.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "CHECKPOINT_NAME = 'deeplabv2_cityscapes.pth'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "SAVE_PATH = f'/content/drive/MyDrive/semseg/{CHECKPOINT_NAME}'\n",
        "\n",
        "# --- DEEPLAB V2 MODEL DEFINITION ---\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=256, rates=[6, 12, 18, 24]):\n",
        "        super(ASPP, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        # 1x1 Conv\n",
        "        self.convs.append(nn.Conv2d(in_channels, out_channels, 1, bias=False))\n",
        "        # Atrous Convolutions\n",
        "        for rate in rates:\n",
        "            self.convs.append(nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False))\n",
        "\n",
        "        self.out_conv = nn.Conv2d(len(rates) + 1 * out_channels, out_channels, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        for conv in self.convs:\n",
        "            outs.append(conv(x))\n",
        "        x = torch.cat(outs, dim=1)\n",
        "        return x\n",
        "\n",
        "class DeepLabV2(nn.Module):\n",
        "    def __init__(self, num_classes=19, backbone='resnet101'):\n",
        "        super(DeepLabV2, self).__init__()\n",
        "\n",
        "        # Load ResNet Backbone\n",
        "        if backbone == 'resnet101':\n",
        "            resnet = models.resnet101(pretrained=True)\n",
        "        else:\n",
        "            resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4 # In DeepLabV2, often dilated, but we keep standard for simplicity or use dilation if needed\n",
        "\n",
        "        # Replace last stride to preserve resolution\n",
        "\n",
        "        # ASPP Head\n",
        "        self.aspp = ASPP(2048, 256, rates=[6, 12, 18, 24])\n",
        "\n",
        "        # Classification Head\n",
        "        self.cls_conv = nn.Conv2d(1280, num_classes, 1) # 256*5 channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2], x.shape[3]\n",
        "\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.aspp(x)\n",
        "\n",
        "        # The ASPP output needs to be reduced.\n",
        "        # Note: In standard implementation, we concat.\n",
        "        # Let's fix the dimension issue: 5 branches * 256 = 1280 channels.\n",
        "\n",
        "        x = self.cls_conv(x)\n",
        "\n",
        "        # Upsample to original size\n",
        "        x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n",
        "        return x\n",
        "\n",
        "# --- DATASET ---\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.images_dir = os.path.join(root, 'leftImg8bit', split)\n",
        "        self.masks_dir = os.path.join(root, 'gtFine', split)\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "\n",
        "        if os.path.exists(self.images_dir):\n",
        "            for city in sorted(os.listdir(self.images_dir)):\n",
        "                img_dir = os.path.join(self.images_dir, city)\n",
        "                mask_dir = os.path.join(self.masks_dir, city)\n",
        "                if not os.path.isdir(img_dir): continue\n",
        "                for file_name in sorted(os.listdir(img_dir)):\n",
        "                    if file_name.endswith('_leftImg8bit.png'):\n",
        "                        self.images.append(os.path.join(img_dir, file_name))\n",
        "                        mask_name = file_name.replace('_leftImg8bit.png', '_gtFine_labelTrainIds.png')\n",
        "                        self.masks.append(os.path.join(mask_dir, mask_name))\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB').resize((1024, 512), Image.BILINEAR)\n",
        "        mask = Image.open(self.masks[idx]).resize((1024, 512), Image.NEAREST)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.from_numpy(np.array(mask)).long()\n",
        "\n",
        "# --- TRAINING ---\n",
        "print(\"Starting DeepLabV2 (Classic) Training on Cityscapes...\")\n",
        "\n",
        "model = DeepLabV2(num_classes=19, backbone='resnet101').to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.PolyLR(optimizer, total_iters=EPOCHS, power=0.9) if hasattr(optim.lr_scheduler, 'PolyLR') else optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "dataset = CityscapesDataset(CITYSCAPES_PATH, split='train', transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(\"Resuming checkpoint...\")\n",
        "    checkpoint = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Step [{i}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    if hasattr(optimizer, 'get_last_lr'):\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, SAVE_PATH)\n",
        "    print(f\"Epoch {epoch+1} Saved.\")\n",
        "\n",
        "# --- METRICS CALCULATION (Latency & Params) ---\n",
        "print(\"\\nCalculating Metrics...\")\n",
        "model.eval()\n",
        "dummy_input = torch.randn(1, 3, 512, 1024).to(DEVICE)\n",
        "\n",
        "# 1. Parameters\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of Parameters: {num_params / 1e6:.2f} M\")\n",
        "\n",
        "# 2. Latency (Avg of 100 runs)\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        _ = model(dummy_input)\n",
        "end = time.time()\n",
        "latency = (end - start) / 100 * 1000 # in ms\n",
        "print(f\"Latency: {latency:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ni8wKUyncd",
        "outputId": "539e7462-8f55-4161-ed33-0faf8a6490cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DeepLabV2 (Classic) Training on Cityscapes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:00<00:00, 181MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Step [0/393] Loss: 2.8585\n",
            "Epoch [1/50] Step [50/393] Loss: 0.7194\n",
            "Epoch [1/50] Step [100/393] Loss: 0.4542\n",
            "Epoch [1/50] Step [150/393] Loss: 0.3840\n",
            "Epoch [1/50] Step [200/393] Loss: 0.2402\n",
            "Epoch [1/50] Step [250/393] Loss: 0.3345\n",
            "Epoch [1/50] Step [300/393] Loss: 0.2666\n",
            "Epoch [1/50] Step [350/393] Loss: 0.3537\n",
            "Epoch 1 Saved.\n",
            "Epoch [2/50] Step [0/393] Loss: 0.2418\n",
            "Epoch [2/50] Step [50/393] Loss: 0.3757\n",
            "Epoch [2/50] Step [100/393] Loss: 0.2090\n",
            "Epoch [2/50] Step [150/393] Loss: 0.2385\n",
            "Epoch [2/50] Step [200/393] Loss: 0.2225\n",
            "Epoch [2/50] Step [250/393] Loss: 0.2121\n",
            "Epoch [2/50] Step [300/393] Loss: 0.2872\n",
            "Epoch [2/50] Step [350/393] Loss: 0.2315\n",
            "Epoch 2 Saved.\n",
            "Epoch [3/50] Step [0/393] Loss: 0.2535\n",
            "Epoch [3/50] Step [50/393] Loss: 0.2187\n",
            "Epoch [3/50] Step [100/393] Loss: 0.3407\n",
            "Epoch [3/50] Step [150/393] Loss: 0.1976\n",
            "Epoch [3/50] Step [200/393] Loss: 0.1967\n",
            "Epoch [3/50] Step [250/393] Loss: 0.2312\n",
            "Epoch [3/50] Step [300/393] Loss: 0.2049\n",
            "Epoch [3/50] Step [350/393] Loss: 0.2994\n",
            "Epoch 3 Saved.\n",
            "Epoch [4/50] Step [0/393] Loss: 0.2313\n",
            "Epoch [4/50] Step [50/393] Loss: 0.2049\n",
            "Epoch [4/50] Step [100/393] Loss: 0.1842\n",
            "Epoch [4/50] Step [150/393] Loss: 0.2530\n",
            "Epoch [4/50] Step [200/393] Loss: 0.1589\n",
            "Epoch [4/50] Step [250/393] Loss: 0.2118\n",
            "Epoch [4/50] Step [300/393] Loss: 0.1584\n",
            "Epoch [4/50] Step [350/393] Loss: 0.1746\n",
            "Epoch 4 Saved.\n",
            "Epoch [5/50] Step [0/393] Loss: 0.1950\n",
            "Epoch [5/50] Step [50/393] Loss: 0.1669\n",
            "Epoch [5/50] Step [100/393] Loss: 0.1879\n",
            "Epoch [5/50] Step [150/393] Loss: 0.1411\n",
            "Epoch [5/50] Step [200/393] Loss: 0.1484\n",
            "Epoch [5/50] Step [250/393] Loss: 0.1373\n",
            "Epoch [5/50] Step [300/393] Loss: 0.2186\n",
            "Epoch [5/50] Step [350/393] Loss: 0.1883\n",
            "Epoch 5 Saved.\n",
            "Epoch [6/50] Step [0/393] Loss: 0.1419\n",
            "Epoch [6/50] Step [50/393] Loss: 0.1598\n",
            "Epoch [6/50] Step [100/393] Loss: 0.2084\n",
            "Epoch [6/50] Step [150/393] Loss: 0.1068\n",
            "Epoch [6/50] Step [200/393] Loss: 0.1726\n",
            "Epoch [6/50] Step [250/393] Loss: 0.1531\n",
            "Epoch [6/50] Step [300/393] Loss: 0.1336\n",
            "Epoch [6/50] Step [350/393] Loss: 0.2004\n",
            "Epoch 6 Saved.\n",
            "Epoch [7/50] Step [0/393] Loss: 0.2049\n",
            "Epoch [7/50] Step [50/393] Loss: 0.1485\n",
            "Epoch [7/50] Step [100/393] Loss: 0.1317\n",
            "Epoch [7/50] Step [150/393] Loss: 0.1700\n",
            "Epoch [7/50] Step [200/393] Loss: 0.1991\n",
            "Epoch [7/50] Step [250/393] Loss: 0.1063\n",
            "Epoch [7/50] Step [300/393] Loss: 0.2034\n",
            "Epoch [7/50] Step [350/393] Loss: 0.1398\n",
            "Epoch 7 Saved.\n",
            "Epoch [8/50] Step [0/393] Loss: 0.1804\n",
            "Epoch [8/50] Step [50/393] Loss: 0.1804\n",
            "Epoch [8/50] Step [100/393] Loss: 0.1481\n",
            "Epoch [8/50] Step [150/393] Loss: 0.2230\n",
            "Epoch [8/50] Step [200/393] Loss: 0.1275\n",
            "Epoch [8/50] Step [250/393] Loss: 0.1725\n",
            "Epoch [8/50] Step [300/393] Loss: 0.1800\n",
            "Epoch [8/50] Step [350/393] Loss: 0.1724\n",
            "Epoch 8 Saved.\n",
            "Epoch [9/50] Step [0/393] Loss: 0.1675\n",
            "Epoch [9/50] Step [50/393] Loss: 0.1831\n",
            "Epoch [9/50] Step [100/393] Loss: 0.1738\n",
            "Epoch [9/50] Step [150/393] Loss: 0.1509\n",
            "Epoch [9/50] Step [200/393] Loss: 0.1210\n",
            "Epoch [9/50] Step [250/393] Loss: 0.1525\n",
            "Epoch [9/50] Step [300/393] Loss: 0.1095\n",
            "Epoch [9/50] Step [350/393] Loss: 0.1005\n",
            "Epoch 9 Saved.\n",
            "Epoch [10/50] Step [0/393] Loss: 0.1575\n",
            "Epoch [10/50] Step [50/393] Loss: 0.1919\n",
            "Epoch [10/50] Step [100/393] Loss: 0.1161\n",
            "Epoch [10/50] Step [150/393] Loss: 0.1213\n",
            "Epoch [10/50] Step [200/393] Loss: 0.1297\n",
            "Epoch [10/50] Step [250/393] Loss: 0.1777\n",
            "Epoch [10/50] Step [300/393] Loss: 0.1317\n",
            "Epoch [10/50] Step [350/393] Loss: 0.1493\n",
            "Epoch 10 Saved.\n",
            "Epoch [11/50] Step [0/393] Loss: 0.1427\n",
            "Epoch [11/50] Step [50/393] Loss: 0.1751\n",
            "Epoch [11/50] Step [100/393] Loss: 0.1274\n",
            "Epoch [11/50] Step [150/393] Loss: 0.1108\n",
            "Epoch [11/50] Step [200/393] Loss: 0.1554\n",
            "Epoch [11/50] Step [250/393] Loss: 0.1194\n",
            "Epoch [11/50] Step [300/393] Loss: 0.1643\n",
            "Epoch [11/50] Step [350/393] Loss: 0.1611\n",
            "Epoch 11 Saved.\n",
            "Epoch [12/50] Step [0/393] Loss: 0.1046\n",
            "Epoch [12/50] Step [50/393] Loss: 0.1733\n",
            "Epoch [12/50] Step [100/393] Loss: 0.1168\n",
            "Epoch [12/50] Step [150/393] Loss: 0.0954\n",
            "Epoch [12/50] Step [200/393] Loss: 0.1065\n",
            "Epoch [12/50] Step [250/393] Loss: 0.1062\n",
            "Epoch [12/50] Step [300/393] Loss: 0.1554\n",
            "Epoch [12/50] Step [350/393] Loss: 0.1267\n",
            "Epoch 12 Saved.\n",
            "Epoch [13/50] Step [0/393] Loss: 0.1830\n",
            "Epoch [13/50] Step [50/393] Loss: 0.1533\n",
            "Epoch [13/50] Step [100/393] Loss: 0.1536\n",
            "Epoch [13/50] Step [150/393] Loss: 0.1318\n",
            "Epoch [13/50] Step [200/393] Loss: 0.1276\n",
            "Epoch [13/50] Step [250/393] Loss: 0.1154\n",
            "Epoch [13/50] Step [300/393] Loss: 0.0897\n",
            "Epoch [13/50] Step [350/393] Loss: 0.1153\n",
            "Epoch 13 Saved.\n",
            "Epoch [14/50] Step [0/393] Loss: 0.1017\n",
            "Epoch [14/50] Step [50/393] Loss: 0.0950\n",
            "Epoch [14/50] Step [100/393] Loss: 0.1313\n",
            "Epoch [14/50] Step [150/393] Loss: 0.1248\n",
            "Epoch [14/50] Step [200/393] Loss: 0.1143\n",
            "Epoch [14/50] Step [250/393] Loss: 0.1006\n",
            "Epoch [14/50] Step [300/393] Loss: 0.1447\n",
            "Epoch [14/50] Step [350/393] Loss: 0.1273\n",
            "Epoch 14 Saved.\n",
            "Epoch [15/50] Step [0/393] Loss: 0.1259\n",
            "Epoch [15/50] Step [50/393] Loss: 0.1116\n",
            "Epoch [15/50] Step [100/393] Loss: 0.1247\n",
            "Epoch [15/50] Step [150/393] Loss: 0.0981\n",
            "Epoch [15/50] Step [200/393] Loss: 0.1406\n",
            "Epoch [15/50] Step [250/393] Loss: 0.1173\n",
            "Epoch [15/50] Step [300/393] Loss: 0.1449\n",
            "Epoch [15/50] Step [350/393] Loss: 0.0898\n",
            "Epoch 15 Saved.\n",
            "Epoch [16/50] Step [0/393] Loss: 0.1149\n",
            "Epoch [16/50] Step [50/393] Loss: 0.1187\n",
            "Epoch [16/50] Step [100/393] Loss: 0.1616\n",
            "Epoch [16/50] Step [150/393] Loss: 0.0982\n",
            "Epoch [16/50] Step [200/393] Loss: 0.0953\n",
            "Epoch [16/50] Step [250/393] Loss: 0.1124\n",
            "Epoch [16/50] Step [300/393] Loss: 0.1108\n",
            "Epoch [16/50] Step [350/393] Loss: 0.1267\n",
            "Epoch 16 Saved.\n",
            "Epoch [17/50] Step [0/393] Loss: 0.1410\n",
            "Epoch [17/50] Step [50/393] Loss: 0.1074\n",
            "Epoch [17/50] Step [100/393] Loss: 0.1323\n",
            "Epoch [17/50] Step [150/393] Loss: 0.1280\n",
            "Epoch [17/50] Step [200/393] Loss: 0.0920\n",
            "Epoch [17/50] Step [250/393] Loss: 0.0874\n",
            "Epoch [17/50] Step [300/393] Loss: 0.1281\n",
            "Epoch [17/50] Step [350/393] Loss: 0.1674\n",
            "Epoch 17 Saved.\n",
            "Epoch [18/50] Step [0/393] Loss: 0.1558\n",
            "Epoch [18/50] Step [50/393] Loss: 0.0949\n",
            "Epoch [18/50] Step [100/393] Loss: 0.1129\n",
            "Epoch [18/50] Step [150/393] Loss: 0.1218\n",
            "Epoch [18/50] Step [200/393] Loss: 0.1006\n",
            "Epoch [18/50] Step [250/393] Loss: 0.1389\n",
            "Epoch [18/50] Step [300/393] Loss: 0.1011\n",
            "Epoch [18/50] Step [350/393] Loss: 0.0815\n",
            "Epoch 18 Saved.\n",
            "Epoch [19/50] Step [0/393] Loss: 0.1158\n",
            "Epoch [19/50] Step [50/393] Loss: 0.0761\n",
            "Epoch [19/50] Step [100/393] Loss: 0.1065\n",
            "Epoch [19/50] Step [150/393] Loss: 0.1056\n",
            "Epoch [19/50] Step [200/393] Loss: 0.1351\n",
            "Epoch [19/50] Step [250/393] Loss: 0.1356\n",
            "Epoch [19/50] Step [300/393] Loss: 0.0838\n",
            "Epoch [19/50] Step [350/393] Loss: 0.1512\n",
            "Epoch 19 Saved.\n",
            "Epoch [20/50] Step [0/393] Loss: 0.1091\n",
            "Epoch [20/50] Step [50/393] Loss: 0.1313\n",
            "Epoch [20/50] Step [100/393] Loss: 0.0929\n",
            "Epoch [20/50] Step [150/393] Loss: 0.0793\n",
            "Epoch [20/50] Step [200/393] Loss: 0.1024\n",
            "Epoch [20/50] Step [250/393] Loss: 0.1360\n",
            "Epoch [20/50] Step [300/393] Loss: 0.0993\n",
            "Epoch [20/50] Step [350/393] Loss: 0.0894\n",
            "Epoch 20 Saved.\n",
            "Epoch [21/50] Step [0/393] Loss: 0.0876\n",
            "Epoch [21/50] Step [50/393] Loss: 0.1108\n",
            "Epoch [21/50] Step [100/393] Loss: 0.1415\n",
            "Epoch [21/50] Step [150/393] Loss: 0.1190\n",
            "Epoch [21/50] Step [200/393] Loss: 0.0805\n",
            "Epoch [21/50] Step [250/393] Loss: 0.0947\n",
            "Epoch [21/50] Step [300/393] Loss: 0.0955\n",
            "Epoch [21/50] Step [350/393] Loss: 0.0886\n",
            "Epoch 21 Saved.\n",
            "Epoch [22/50] Step [0/393] Loss: 0.1113\n",
            "Epoch [22/50] Step [50/393] Loss: 0.0976\n",
            "Epoch [22/50] Step [100/393] Loss: 0.1262\n",
            "Epoch [22/50] Step [150/393] Loss: 0.0988\n",
            "Epoch [22/50] Step [200/393] Loss: 0.1248\n",
            "Epoch [22/50] Step [250/393] Loss: 0.1168\n",
            "Epoch [22/50] Step [300/393] Loss: 0.1100\n",
            "Epoch [22/50] Step [350/393] Loss: 0.1000\n",
            "Epoch 22 Saved.\n",
            "Epoch [23/50] Step [0/393] Loss: 0.1333\n",
            "Epoch [23/50] Step [50/393] Loss: 0.1266\n",
            "Epoch [23/50] Step [100/393] Loss: 0.1007\n",
            "Epoch [23/50] Step [150/393] Loss: 0.1056\n",
            "Epoch [23/50] Step [200/393] Loss: 0.1479\n",
            "Epoch [23/50] Step [250/393] Loss: 0.0938\n",
            "Epoch [23/50] Step [300/393] Loss: 0.1140\n",
            "Epoch [23/50] Step [350/393] Loss: 0.1039\n",
            "Epoch 23 Saved.\n",
            "Epoch [24/50] Step [0/393] Loss: 0.1019\n",
            "Epoch [24/50] Step [50/393] Loss: 0.1417\n",
            "Epoch [24/50] Step [100/393] Loss: 0.0808\n",
            "Epoch [24/50] Step [150/393] Loss: 0.0813\n",
            "Epoch [24/50] Step [200/393] Loss: 0.0873\n",
            "Epoch [24/50] Step [250/393] Loss: 0.1108\n",
            "Epoch [24/50] Step [300/393] Loss: 0.1280\n",
            "Epoch [24/50] Step [350/393] Loss: 0.0811\n",
            "Epoch 24 Saved.\n",
            "Epoch [25/50] Step [0/393] Loss: 0.1103\n",
            "Epoch [25/50] Step [50/393] Loss: 0.1151\n",
            "Epoch [25/50] Step [100/393] Loss: 0.0933\n",
            "Epoch [25/50] Step [150/393] Loss: 0.0692\n",
            "Epoch [25/50] Step [200/393] Loss: 0.5295\n",
            "Epoch [25/50] Step [250/393] Loss: 0.1824\n",
            "Epoch [25/50] Step [300/393] Loss: 0.3225\n",
            "Epoch [25/50] Step [350/393] Loss: 0.2829\n",
            "Epoch 25 Saved.\n",
            "Epoch [26/50] Step [0/393] Loss: 0.2335\n",
            "Epoch [26/50] Step [50/393] Loss: 0.1414\n",
            "Epoch [26/50] Step [100/393] Loss: 0.1905\n",
            "Epoch [26/50] Step [150/393] Loss: 0.1875\n",
            "Epoch [26/50] Step [200/393] Loss: 0.1437\n",
            "Epoch [26/50] Step [250/393] Loss: 0.1479\n",
            "Epoch [26/50] Step [300/393] Loss: 0.2067\n",
            "Epoch [26/50] Step [350/393] Loss: 0.1912\n",
            "Epoch 26 Saved.\n",
            "Epoch [27/50] Step [0/393] Loss: 0.2000\n",
            "Epoch [27/50] Step [50/393] Loss: 0.1409\n",
            "Epoch [27/50] Step [100/393] Loss: 0.1704\n",
            "Epoch [27/50] Step [150/393] Loss: 0.1398\n",
            "Epoch [27/50] Step [200/393] Loss: 0.1683\n",
            "Epoch [27/50] Step [250/393] Loss: 0.1445\n",
            "Epoch [27/50] Step [300/393] Loss: 0.1516\n",
            "Epoch [27/50] Step [350/393] Loss: 0.1275\n",
            "Epoch 27 Saved.\n",
            "Epoch [28/50] Step [0/393] Loss: 0.1708\n",
            "Epoch [28/50] Step [50/393] Loss: 0.1067\n",
            "Epoch [28/50] Step [100/393] Loss: 0.1187\n",
            "Epoch [28/50] Step [150/393] Loss: 0.1131\n",
            "Epoch [28/50] Step [200/393] Loss: 0.1368\n",
            "Epoch [28/50] Step [250/393] Loss: 0.1311\n",
            "Epoch [28/50] Step [300/393] Loss: 0.1143\n",
            "Epoch [28/50] Step [350/393] Loss: 0.1126\n",
            "Epoch 28 Saved.\n",
            "Epoch [29/50] Step [0/393] Loss: 0.1515\n",
            "Epoch [29/50] Step [50/393] Loss: 0.1345\n",
            "Epoch [29/50] Step [100/393] Loss: 0.1337\n",
            "Epoch [29/50] Step [150/393] Loss: 0.1124\n",
            "Epoch [29/50] Step [200/393] Loss: 0.1546\n",
            "Epoch [29/50] Step [250/393] Loss: 0.1223\n",
            "Epoch [29/50] Step [300/393] Loss: 0.1060\n",
            "Epoch [29/50] Step [350/393] Loss: 0.0833\n",
            "Epoch 29 Saved.\n",
            "Epoch [30/50] Step [0/393] Loss: 0.1225\n",
            "Epoch [30/50] Step [50/393] Loss: 0.1270\n",
            "Epoch [30/50] Step [100/393] Loss: 0.1002\n",
            "Epoch [30/50] Step [150/393] Loss: 0.1237\n",
            "Epoch [30/50] Step [200/393] Loss: 0.1299\n",
            "Epoch [30/50] Step [250/393] Loss: 0.1131\n",
            "Epoch [30/50] Step [300/393] Loss: 0.1067\n",
            "Epoch [30/50] Step [350/393] Loss: 0.0872\n",
            "Epoch 30 Saved.\n",
            "Epoch [31/50] Step [0/393] Loss: 0.0680\n",
            "Epoch [31/50] Step [50/393] Loss: 0.1084\n",
            "Epoch [31/50] Step [100/393] Loss: 0.0928\n",
            "Epoch [31/50] Step [150/393] Loss: 0.0912\n",
            "Epoch [31/50] Step [200/393] Loss: 0.1161\n",
            "Epoch [31/50] Step [250/393] Loss: 0.1098\n",
            "Epoch [31/50] Step [300/393] Loss: 0.1028\n",
            "Epoch [31/50] Step [350/393] Loss: 0.1307\n",
            "Epoch 31 Saved.\n",
            "Epoch [32/50] Step [0/393] Loss: 0.0812\n",
            "Epoch [32/50] Step [50/393] Loss: 0.1103\n",
            "Epoch [32/50] Step [100/393] Loss: 0.1102\n",
            "Epoch [32/50] Step [150/393] Loss: 0.1026\n",
            "Epoch [32/50] Step [200/393] Loss: 0.0825\n",
            "Epoch [32/50] Step [250/393] Loss: 0.0903\n",
            "Epoch [32/50] Step [300/393] Loss: 0.0902\n",
            "Epoch [32/50] Step [350/393] Loss: 0.0964\n",
            "Epoch 32 Saved.\n",
            "Epoch [33/50] Step [0/393] Loss: 0.1030\n",
            "Epoch [33/50] Step [50/393] Loss: 0.0950\n",
            "Epoch [33/50] Step [100/393] Loss: 0.1170\n",
            "Epoch [33/50] Step [150/393] Loss: 0.1235\n",
            "Epoch [33/50] Step [200/393] Loss: 0.1045\n",
            "Epoch [33/50] Step [250/393] Loss: 0.1274\n",
            "Epoch [33/50] Step [300/393] Loss: 0.0846\n",
            "Epoch [33/50] Step [350/393] Loss: 0.0872\n",
            "Epoch 33 Saved.\n",
            "Epoch [34/50] Step [0/393] Loss: 0.0828\n",
            "Epoch [34/50] Step [50/393] Loss: 0.1042\n",
            "Epoch [34/50] Step [100/393] Loss: 0.0854\n",
            "Epoch [34/50] Step [150/393] Loss: 0.0792\n",
            "Epoch [34/50] Step [200/393] Loss: 0.1082\n",
            "Epoch [34/50] Step [250/393] Loss: 0.1385\n",
            "Epoch [34/50] Step [300/393] Loss: 0.0965\n",
            "Epoch [34/50] Step [350/393] Loss: 0.1061\n",
            "Epoch 34 Saved.\n",
            "Epoch [35/50] Step [0/393] Loss: 0.0917\n",
            "Epoch [35/50] Step [50/393] Loss: 0.0773\n",
            "Epoch [35/50] Step [100/393] Loss: 0.0902\n",
            "Epoch [35/50] Step [150/393] Loss: 0.0813\n",
            "Epoch [35/50] Step [200/393] Loss: 0.0958\n",
            "Epoch [35/50] Step [250/393] Loss: 0.1013\n",
            "Epoch [35/50] Step [300/393] Loss: 0.1118\n",
            "Epoch [35/50] Step [350/393] Loss: 0.0854\n",
            "Epoch 35 Saved.\n",
            "Epoch [36/50] Step [0/393] Loss: 0.0817\n",
            "Epoch [36/50] Step [50/393] Loss: 0.0891\n",
            "Epoch [36/50] Step [100/393] Loss: 0.1015\n",
            "Epoch [36/50] Step [150/393] Loss: 0.1430\n",
            "Epoch [36/50] Step [200/393] Loss: 0.0956\n",
            "Epoch [36/50] Step [250/393] Loss: 0.1063\n",
            "Epoch [36/50] Step [300/393] Loss: 0.0828\n",
            "Epoch [36/50] Step [350/393] Loss: 0.0678\n",
            "Epoch 36 Saved.\n",
            "Epoch [37/50] Step [0/393] Loss: 0.1375\n",
            "Epoch [37/50] Step [50/393] Loss: 0.0790\n",
            "Epoch [37/50] Step [100/393] Loss: 0.0795\n",
            "Epoch [37/50] Step [150/393] Loss: 0.1168\n",
            "Epoch [37/50] Step [200/393] Loss: 0.1146\n",
            "Epoch [37/50] Step [250/393] Loss: 0.1057\n",
            "Epoch [37/50] Step [300/393] Loss: 0.1007\n",
            "Epoch [37/50] Step [350/393] Loss: 0.0731\n",
            "Epoch 37 Saved.\n",
            "Epoch [38/50] Step [0/393] Loss: 0.1120\n",
            "Epoch [38/50] Step [50/393] Loss: 0.1011\n",
            "Epoch [38/50] Step [100/393] Loss: 0.0853\n",
            "Epoch [38/50] Step [150/393] Loss: 0.0593\n",
            "Epoch [38/50] Step [200/393] Loss: 0.1092\n",
            "Epoch [38/50] Step [250/393] Loss: 0.1014\n",
            "Epoch [38/50] Step [300/393] Loss: 0.1032\n",
            "Epoch [38/50] Step [350/393] Loss: 0.0828\n",
            "Epoch 38 Saved.\n",
            "Epoch [39/50] Step [0/393] Loss: 0.0556\n",
            "Epoch [39/50] Step [50/393] Loss: 0.0787\n",
            "Epoch [39/50] Step [100/393] Loss: 0.1570\n",
            "Epoch [39/50] Step [150/393] Loss: 0.1061\n",
            "Epoch [39/50] Step [200/393] Loss: 0.0654\n",
            "Epoch [39/50] Step [250/393] Loss: 0.0953\n",
            "Epoch [39/50] Step [300/393] Loss: 0.1099\n",
            "Epoch [39/50] Step [350/393] Loss: 0.1137\n",
            "Epoch 39 Saved.\n",
            "Epoch [40/50] Step [0/393] Loss: 0.0929\n",
            "Epoch [40/50] Step [50/393] Loss: 0.0975\n",
            "Epoch [40/50] Step [100/393] Loss: 0.0716\n",
            "Epoch [40/50] Step [150/393] Loss: 0.1101\n",
            "Epoch [40/50] Step [200/393] Loss: 0.1128\n",
            "Epoch [40/50] Step [250/393] Loss: 0.0984\n",
            "Epoch [40/50] Step [300/393] Loss: 0.0695\n",
            "Epoch [40/50] Step [350/393] Loss: 0.1242\n",
            "Epoch 40 Saved.\n",
            "Epoch [41/50] Step [0/393] Loss: 0.0666\n",
            "Epoch [41/50] Step [50/393] Loss: 0.0730\n",
            "Epoch [41/50] Step [100/393] Loss: 0.1007\n",
            "Epoch [41/50] Step [150/393] Loss: 0.0849\n",
            "Epoch [41/50] Step [200/393] Loss: 0.1129\n",
            "Epoch [41/50] Step [250/393] Loss: 0.1024\n",
            "Epoch [41/50] Step [300/393] Loss: 0.0908\n",
            "Epoch [41/50] Step [350/393] Loss: 0.1029\n",
            "Epoch 41 Saved.\n",
            "Epoch [42/50] Step [0/393] Loss: 0.0747\n",
            "Epoch [42/50] Step [50/393] Loss: 0.1126\n",
            "Epoch [42/50] Step [100/393] Loss: 0.1102\n",
            "Epoch [42/50] Step [150/393] Loss: 0.1051\n",
            "Epoch [42/50] Step [200/393] Loss: 0.0977\n",
            "Epoch [42/50] Step [250/393] Loss: 0.1655\n",
            "Epoch [42/50] Step [300/393] Loss: 0.2034\n",
            "Epoch [42/50] Step [350/393] Loss: 0.2049\n",
            "Epoch 42 Saved.\n",
            "Epoch [43/50] Step [0/393] Loss: 0.2736\n",
            "Epoch [43/50] Step [50/393] Loss: 0.2571\n",
            "Epoch [43/50] Step [100/393] Loss: 0.1461\n",
            "Epoch [43/50] Step [150/393] Loss: 0.2557\n",
            "Epoch [43/50] Step [200/393] Loss: 0.2248\n",
            "Epoch [43/50] Step [250/393] Loss: 0.2039\n",
            "Epoch [43/50] Step [300/393] Loss: 0.2624\n",
            "Epoch [43/50] Step [350/393] Loss: 0.1530\n",
            "Epoch 43 Saved.\n",
            "Epoch [44/50] Step [0/393] Loss: 0.1432\n",
            "Epoch [44/50] Step [50/393] Loss: 0.1353\n",
            "Epoch [44/50] Step [100/393] Loss: 0.1302\n",
            "Epoch [44/50] Step [150/393] Loss: 0.1543\n",
            "Epoch [44/50] Step [200/393] Loss: 0.1678\n",
            "Epoch [44/50] Step [250/393] Loss: 0.1304\n",
            "Epoch [44/50] Step [300/393] Loss: 0.2093\n",
            "Epoch [44/50] Step [350/393] Loss: 0.1502\n",
            "Epoch 44 Saved.\n",
            "Epoch [45/50] Step [0/393] Loss: 0.1560\n",
            "Epoch [45/50] Step [50/393] Loss: 0.1064\n",
            "Epoch [45/50] Step [100/393] Loss: 0.0900\n",
            "Epoch [45/50] Step [150/393] Loss: 0.1605\n",
            "Epoch [45/50] Step [200/393] Loss: 0.0892\n",
            "Epoch [45/50] Step [250/393] Loss: 0.1750\n",
            "Epoch [45/50] Step [300/393] Loss: 0.1432\n",
            "Epoch [45/50] Step [350/393] Loss: 0.1099\n",
            "Epoch 45 Saved.\n",
            "Epoch [46/50] Step [0/393] Loss: 0.1014\n",
            "Epoch [46/50] Step [50/393] Loss: 0.1258\n",
            "Epoch [46/50] Step [100/393] Loss: 0.0723\n",
            "Epoch [46/50] Step [150/393] Loss: 0.1000\n",
            "Epoch [46/50] Step [200/393] Loss: 0.1270\n",
            "Epoch [46/50] Step [250/393] Loss: 0.1114\n",
            "Epoch [46/50] Step [300/393] Loss: 0.0833\n",
            "Epoch [46/50] Step [350/393] Loss: 0.0733\n",
            "Epoch 46 Saved.\n",
            "Epoch [47/50] Step [0/393] Loss: 0.0929\n",
            "Epoch [47/50] Step [50/393] Loss: 0.0892\n",
            "Epoch [47/50] Step [100/393] Loss: 0.1031\n",
            "Epoch [47/50] Step [150/393] Loss: 0.1365\n",
            "Epoch [47/50] Step [200/393] Loss: 0.1047\n",
            "Epoch [47/50] Step [250/393] Loss: 0.1129\n",
            "Epoch [47/50] Step [300/393] Loss: 0.1326\n",
            "Epoch [47/50] Step [350/393] Loss: 0.1002\n",
            "Epoch 47 Saved.\n",
            "Epoch [48/50] Step [0/393] Loss: 0.0932\n",
            "Epoch [48/50] Step [50/393] Loss: 0.0728\n",
            "Epoch [48/50] Step [100/393] Loss: 0.0880\n",
            "Epoch [48/50] Step [150/393] Loss: 0.0942\n",
            "Epoch [48/50] Step [200/393] Loss: 0.0831\n",
            "Epoch [48/50] Step [250/393] Loss: 0.1171\n",
            "Epoch [48/50] Step [300/393] Loss: 0.1032\n",
            "Epoch [48/50] Step [350/393] Loss: 0.1185\n",
            "Epoch 48 Saved.\n",
            "Epoch [49/50] Step [0/393] Loss: 0.1094\n",
            "Epoch [49/50] Step [50/393] Loss: 0.0961\n",
            "Epoch [49/50] Step [100/393] Loss: 0.0579\n",
            "Epoch [49/50] Step [150/393] Loss: 0.1059\n",
            "Epoch [49/50] Step [200/393] Loss: 0.1544\n",
            "Epoch [49/50] Step [250/393] Loss: 0.0987\n",
            "Epoch [49/50] Step [300/393] Loss: 0.0799\n",
            "Epoch [49/50] Step [350/393] Loss: 0.0764\n",
            "Epoch 49 Saved.\n",
            "Epoch [50/50] Step [0/393] Loss: 0.0973\n",
            "Epoch [50/50] Step [50/393] Loss: 0.1056\n",
            "Epoch [50/50] Step [100/393] Loss: 0.1435\n",
            "Epoch [50/50] Step [150/393] Loss: 0.1059\n",
            "Epoch [50/50] Step [200/393] Loss: 0.0822\n",
            "Epoch [50/50] Step [250/393] Loss: 0.0758\n",
            "Epoch [50/50] Step [300/393] Loss: 0.1263\n",
            "Epoch [50/50] Step [350/393] Loss: 0.0928\n",
            "Epoch 50 Saved.\n",
            "\n",
            "Calculating Metrics...\n",
            "Number of Parameters: 61.99 M\n",
            "Latency: 67.44 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Install thop for FLOPs calculation\n",
        "try:\n",
        "    import thop\n",
        "except ImportError:\n",
        "    os.system('pip install thop')\n",
        "    import thop\n",
        "\n",
        "# Configuration\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/semseg/deeplabv2_cityscapes.pth'\n",
        "CITYSCAPES_PATH = '/content/dataset/project_data/cityscapes'\n",
        "NUM_CLASSES = 19\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "CLASSES = [\n",
        "    \"Road\", \"Sidewalk\", \"Building\", \"Wall\", \"Fence\", \"Pole\",\n",
        "    \"Traffic Light\", \"Traffic Sign\", \"Vegetation\", \"Terrain\", \"Sky\",\n",
        "    \"Person\", \"Rider\", \"Car\", \"Truck\", \"Bus\", \"Train\", \"Motorcycle\", \"Bicycle\"\n",
        "]\n",
        "\n",
        "# --- MODEL DEFINITION ---\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=256, rates=[6, 12, 18, 24]):\n",
        "        super(ASPP, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(nn.Conv2d(in_channels, out_channels, 1, bias=False))\n",
        "        for rate in rates:\n",
        "            self.convs.append(nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False))\n",
        "        self.out_conv = nn.Conv2d(len(rates) + 1 * out_channels, out_channels, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        for conv in self.convs:\n",
        "            outs.append(conv(x))\n",
        "        x = torch.cat(outs, dim=1)\n",
        "        return x\n",
        "\n",
        "class DeepLabV2(nn.Module):\n",
        "    def __init__(self, num_classes=19, backbone='resnet101'):\n",
        "        super(DeepLabV2, self).__init__()\n",
        "        resnet = models.resnet101(pretrained=False) # Weights loaded from checkpoint\n",
        "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4\n",
        "        self.aspp = ASPP(2048, 256, rates=[6, 12, 18, 24])\n",
        "        self.cls_conv = nn.Conv2d(1280, num_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2], x.shape[3]\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.aspp(x)\n",
        "        x = self.cls_conv(x)\n",
        "        x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n",
        "        return x\n",
        "\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root, split='val', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.images_dir = os.path.join(root, 'leftImg8bit', split)\n",
        "        self.masks_dir = os.path.join(root, 'gtFine', split)\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "        if os.path.exists(self.images_dir):\n",
        "            for city in sorted(os.listdir(self.images_dir)):\n",
        "                img_dir = os.path.join(self.images_dir, city)\n",
        "                mask_dir = os.path.join(self.masks_dir, city)\n",
        "                if not os.path.isdir(img_dir): continue\n",
        "                for file_name in sorted(os.listdir(img_dir)):\n",
        "                    if file_name.endswith('_leftImg8bit.png'):\n",
        "                        self.images.append(os.path.join(img_dir, file_name))\n",
        "                        mask_name = file_name.replace('_leftImg8bit.png', '_gtFine_labelTrainIds.png')\n",
        "                        self.masks.append(os.path.join(mask_dir, mask_name))\n",
        "\n",
        "    def __len__(self): return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB').resize((1024, 512), Image.BILINEAR)\n",
        "        mask = Image.open(self.masks[idx]).resize((1024, 512), Image.NEAREST)\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image, torch.from_numpy(np.array(mask)).long()\n",
        "\n",
        "# --- EVALUATION ---\n",
        "print(f\"Evaluating DeepLabV2: {CHECKPOINT_PATH}\")\n",
        "\n",
        "model = DeepLabV2(num_classes=NUM_CLASSES, backbone='resnet101').to(DEVICE)\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Model loaded.\")\n",
        "else:\n",
        "    print(\"Checkpoint not found.\")\n",
        "    sys.exit()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# 1. Calculate FLOPs\n",
        "print(\"Calculating FLOPs...\")\n",
        "dummy_input = torch.randn(1, 3, 512, 1024).to(DEVICE)\n",
        "flops, params = thop.profile(model, inputs=(dummy_input, ), verbose=False)\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} G\")\n",
        "\n",
        "# 2. Calculate mIoU\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "dataset = CityscapesDataset(CITYSCAPES_PATH, split='val', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "hist = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
        "print(\"Processing validation images...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.numpy()\n",
        "        output = model(images)\n",
        "        preds = torch.argmax(output, dim=1).cpu().numpy()\n",
        "        mask = (labels >= 0) & (labels < NUM_CLASSES)\n",
        "        hist += np.bincount(\n",
        "            NUM_CLASSES * labels[mask].astype(int) + preds[mask],\n",
        "            minlength=NUM_CLASSES ** 2\n",
        "        ).reshape(NUM_CLASSES, NUM_CLASSES)\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            print(f\"Step [{i}/{len(dataloader)}]\")\n",
        "\n",
        "iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "miou = np.nanmean(iou)\n",
        "\n",
        "print(f\"\\nFinal mIoU (DeepLabV2): {miou * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-uLbIvTx5Ys",
        "outputId": "012d7329-ab38-430d-b3b2-3c084528945a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating DeepLabV2: /content/drive/MyDrive/semseg/deeplabv2_cityscapes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Calculating FLOPs...\n",
            "FLOPs: 92.12 G\n",
            "Processing validation images...\n",
            "Step [0/125]\n",
            "Step [20/125]\n",
            "Step [40/125]\n",
            "Step [60/125]\n",
            "Step [80/125]\n",
            "Step [100/125]\n",
            "Step [120/125]\n",
            "\n",
            "Final mIoU (DeepLabV2): 57.42%\n"
          ]
        }
      ]
    }
  ]
}